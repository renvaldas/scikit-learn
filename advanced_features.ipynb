{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Python version: 3.5.2\n",
      "IPython version: 4.0.1\n",
      "numpy version: 1.13.1\n",
      "scikit-learn version: 0.18.2\n",
      "matplotlib version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import platform\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters we have studied several algorithms for very different tasks,\n",
    "from classification and regression to clustering and dimensionality reduction. We\n",
    "showed how we can apply these algorithms to predict results when faced with new\n",
    "data. That is what machine learning is all about. In this last chapter, we want to show\n",
    "some important concepts and methods you should take into account if you want to\n",
    "do real-world machine learning.\n",
    "- In real-world problems, usually data is not already expressed by attribute/\n",
    "float value pairs, but through more complex structures or is not structured at\n",
    "all. We will learn __feature extraction__ techniques that will allow us to extract\n",
    "scikit-learn features from data.\n",
    "- From the initial set of available features, not all of them will be useful\n",
    "for our algorithms to learn from; in fact, some of them may degrade our\n",
    "performance. We will address the problem of selecting the most adequate\n",
    "feature set, a process known as __feature selection__.\n",
    "- Finally, as we have seen in the examples in this book, many of the machine\n",
    "learning algorithms have parameters that must be set in order to use them.\n",
    "To do that, we will review __model selection__ techniques; that is, methods to\n",
    "select the most promising hyperparameters to our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...the source data does not usually come in this format. We have to\n",
    "extract what we think are potentially useful features and convert them to our learning\n",
    "format. This process is called feature extraction or feature engineering, and it is an\n",
    "often underestimated but very important and time-consuming phase in most realworld\n",
    "machine learning tasks. We can identify two different steps in this task:\n",
    " - __Obtain features__: This step involves processing the source data and extracting\n",
    "the learning instances, usually in the form of feature/value pairs where\n",
    "the value can be an integer or float value, a string, a categorical value, and\n",
    "so on. The method used for extraction depends heavily on how the data\n",
    "is presented. For example, we can have a set of pictures and generate an\n",
    "integer-valued feature for each pixel, indicating its color level, as we did\n",
    "in the face recognition example in Chapter 2, Supervised Learning. Since this\n",
    "is a very task-dependent job, we will not delve into details and assume we\n",
    "already have this setting for our examples.\n",
    " - __Convert features__: Most scikit-learn algorithms assume as an input a set of\n",
    "instances represented as a list of float-valued features. How to get these\n",
    "features will be the main subject of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, as we did in Chapter 2, Supervised Learning, build ad hoc procedures to\n",
    "convert the source data. There are, however, tools that can help us to obtain a\n",
    "suitable representation. The Python package __pandas__ (http://pandas.pydata.\n",
    "org/), for example, provides data structures and tools for data analysis. It aims to\n",
    "provide similar features to those of R, the popular language and environment for\n",
    "statistical computing. We will use pandas to import the Titanic data we presented in\n",
    "Chapter 2, Supervised Learning, and convert them to the scikit-learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names pclass  survived  \\\n",
      "0          1    1st         1   \n",
      "1          2    1st         0   \n",
      "2          3    1st         0   \n",
      "3          4    1st         0   \n",
      "4          5    1st         1   \n",
      "\n",
      "                                              name      age     embarked  \\\n",
      "0                     Allen, Miss Elisabeth Walton  29.0000  Southampton   \n",
      "1                      Allison, Miss Helen Loraine   2.0000  Southampton   \n",
      "2              Allison, Mr Hudson Joshua Creighton  30.0000  Southampton   \n",
      "3  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)  25.0000  Southampton   \n",
      "4                    Allison, Master Hudson Trevor   0.9167  Southampton   \n",
      "\n",
      "                         home.dest room      ticket   boat     sex  \n",
      "0                     St Louis, MO  B-5  24160 L221      2  female  \n",
      "1  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "2  Montreal, PQ / Chesterville, ON  C26         NaN  (135)    male  \n",
      "3  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "4  Montreal, PQ / Chesterville, ON  C22         NaN     11    male  \n"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "print (titanic[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pclass  survived      age     embarked   boat     sex\n",
      "0    1st         1  29.0000  Southampton      2  female\n",
      "1    1st         0   2.0000  Southampton    NaN  female\n",
      "2    1st         0  30.0000  Southampton  (135)    male\n",
      "3    1st         0  25.0000  Southampton    NaN  female\n",
      "4    1st         1   0.9167  Southampton     11    male\n"
     ]
    }
   ],
   "source": [
    "print (titanic.head()[['pclass', 'survived', 'age', 'embarked',\n",
    "'boat', 'sex']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty we have now is that scikit-learn methods expect real numbers\n",
    "as feature values. In Chapter 2, Supervised Learning, we used the LabelEncoder and\n",
    "OneHotEncoder preprocessing methods to manually convert certain categorical\n",
    "features into 1-of-K values (generating a new feature for each possible value; valued\n",
    "1 if the original feature had the corresponding value and 0 otherwise). This time, we\n",
    "will use a similar scikit-learn method, __DictVectorizer__, which automatically builds\n",
    "these features from the different original feature values. Moreover, we will program\n",
    "a method to encode a set of columns in a unique step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "def one_hot_dataframe(data, cols, replace=False):\n",
    "    vec = feature_extraction.DictVectorizer()\n",
    "    mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
    "    vecData = pd.DataFrame(vec.fit_transform( data[cols].apply(mkdict, axis=1)).toarray())\n",
    "    vecData.columns = vec.get_feature_names()\n",
    "    vecData.index = data.index\n",
    "    if replace:\n",
    "        data = data.drop(cols, axis=1)\n",
    "        data = data.join(vecData)\n",
    "    return (data, vecData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one_hot_dataframe method (based on the script at https://gist.github.\n",
    "com/kljensen/5452382) takes a pandas DataFrame data structure and a list of\n",
    "columns and encodes each column into the necessary 1-of-K features. If the replace\n",
    "parameter is True, it will also substitute the original column with the new set. Let's\n",
    "see it applied to the categorical pclass, embarked, and sex features (titanic_n only\n",
    "contains the previously created columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked=Cherbourg</th>\n",
       "      <th>embarked=Queenstown</th>\n",
       "      <th>embarked=Southampton</th>\n",
       "      <th>pclass=1st</th>\n",
       "      <th>pclass=2nd</th>\n",
       "      <th>pclass=3rd</th>\n",
       "      <th>sex=female</th>\n",
       "      <th>sex=male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>821</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.341965</td>\n",
       "      <td>31.194181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154608</td>\n",
       "      <td>0.034273</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.245240</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>0.352628</td>\n",
       "      <td>0.647372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>379.174762</td>\n",
       "      <td>0.474549</td>\n",
       "      <td>14.747525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361668</td>\n",
       "      <td>0.181998</td>\n",
       "      <td>0.496128</td>\n",
       "      <td>0.430393</td>\n",
       "      <td>0.409760</td>\n",
       "      <td>0.498464</td>\n",
       "      <td>0.477970</td>\n",
       "      <td>0.477970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>329.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         row.names     survived         age  embarked  embarked=Cherbourg  \\\n",
       "count  1313.000000  1313.000000  633.000000       821         1313.000000   \n",
       "mean    657.000000     0.341965   31.194181         0            0.154608   \n",
       "std     379.174762     0.474549   14.747525         0            0.361668   \n",
       "min       1.000000     0.000000    0.166700         0            0.000000   \n",
       "25%     329.000000     0.000000   21.000000         0            0.000000   \n",
       "50%     657.000000     0.000000   30.000000         0            0.000000   \n",
       "75%     985.000000     1.000000   41.000000         0            0.000000   \n",
       "max    1313.000000     1.000000   71.000000         0            1.000000   \n",
       "\n",
       "       embarked=Queenstown  embarked=Southampton   pclass=1st   pclass=2nd  \\\n",
       "count          1313.000000           1313.000000  1313.000000  1313.000000   \n",
       "mean              0.034273              0.436405     0.245240     0.213252   \n",
       "std               0.181998              0.496128     0.430393     0.409760   \n",
       "min               0.000000              0.000000     0.000000     0.000000   \n",
       "25%               0.000000              0.000000     0.000000     0.000000   \n",
       "50%               0.000000              0.000000     0.000000     0.000000   \n",
       "75%               0.000000              1.000000     0.000000     0.000000   \n",
       "max               1.000000              1.000000     1.000000     1.000000   \n",
       "\n",
       "        pclass=3rd   sex=female     sex=male  \n",
       "count  1313.000000  1313.000000  1313.000000  \n",
       "mean      0.541508     0.352628     0.647372  \n",
       "std       0.498464     0.477970     0.477970  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       1.000000     0.000000     1.000000  \n",
       "75%       1.000000     1.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic,titanic_n = one_hot_dataframe(titanic, ['pclass', 'embarked', 'sex'], replace=True)\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pclass attribute has been converted to three pclass=1st, pclass=2nd,\n",
    "pclass=3rd features, and similarly for the other two features. Note that the\n",
    "embarked feature has not disappeared, This is due to the fact that the original\n",
    "embarked attribute included NaN values, indicating a missing value; in those cases,\n",
    "every feature based on embarked will be valued 0, but the original feature whose\n",
    "value is NaN remains, indicating the feature is missing for certain instances. Next, we\n",
    "encode the remaining categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic, titanic_n = one_hot_dataframe(titanic, ['home.dest', 'room', 'ticket', 'boat'], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked=Cherbourg</th>\n",
       "      <th>embarked=Queenstown</th>\n",
       "      <th>embarked=Southampton</th>\n",
       "      <th>pclass=1st</th>\n",
       "      <th>pclass=2nd</th>\n",
       "      <th>pclass=3rd</th>\n",
       "      <th>...</th>\n",
       "      <th>ticket=248744 L13</th>\n",
       "      <th>ticket=248749 L13</th>\n",
       "      <th>ticket=250647</th>\n",
       "      <th>ticket=27849</th>\n",
       "      <th>ticket=28220 L32 10s</th>\n",
       "      <th>ticket=34218 L10 10s</th>\n",
       "      <th>ticket=36973 L83 9s 6d</th>\n",
       "      <th>ticket=392091</th>\n",
       "      <th>ticket=7076</th>\n",
       "      <th>ticket=L15 1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>821</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.341965</td>\n",
       "      <td>31.194181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154608</td>\n",
       "      <td>0.034273</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.245240</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>379.174762</td>\n",
       "      <td>0.474549</td>\n",
       "      <td>14.747525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361668</td>\n",
       "      <td>0.181998</td>\n",
       "      <td>0.496128</td>\n",
       "      <td>0.430393</td>\n",
       "      <td>0.409760</td>\n",
       "      <td>0.498464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.047764</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>329.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 580 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         row.names     survived         age  embarked  embarked=Cherbourg  \\\n",
       "count  1313.000000  1313.000000  633.000000       821         1313.000000   \n",
       "mean    657.000000     0.341965   31.194181         0            0.154608   \n",
       "std     379.174762     0.474549   14.747525         0            0.361668   \n",
       "min       1.000000     0.000000    0.166700         0            0.000000   \n",
       "25%     329.000000     0.000000   21.000000         0            0.000000   \n",
       "50%     657.000000     0.000000   30.000000         0            0.000000   \n",
       "75%     985.000000     1.000000   41.000000         0            0.000000   \n",
       "max    1313.000000     1.000000   71.000000         0            1.000000   \n",
       "\n",
       "       embarked=Queenstown  embarked=Southampton   pclass=1st   pclass=2nd  \\\n",
       "count          1313.000000           1313.000000  1313.000000  1313.000000   \n",
       "mean              0.034273              0.436405     0.245240     0.213252   \n",
       "std               0.181998              0.496128     0.430393     0.409760   \n",
       "min               0.000000              0.000000     0.000000     0.000000   \n",
       "25%               0.000000              0.000000     0.000000     0.000000   \n",
       "50%               0.000000              0.000000     0.000000     0.000000   \n",
       "75%               0.000000              1.000000     0.000000     0.000000   \n",
       "max               1.000000              1.000000     1.000000     1.000000   \n",
       "\n",
       "        pclass=3rd      ...        ticket=248744 L13  ticket=248749 L13  \\\n",
       "count  1313.000000      ...              1313.000000        1313.000000   \n",
       "mean      0.541508      ...                 0.000762           0.000762   \n",
       "std       0.498464      ...                 0.027597           0.027597   \n",
       "min       0.000000      ...                 0.000000           0.000000   \n",
       "25%       0.000000      ...                 0.000000           0.000000   \n",
       "50%       1.000000      ...                 0.000000           0.000000   \n",
       "75%       1.000000      ...                 0.000000           0.000000   \n",
       "max       1.000000      ...                 1.000000           1.000000   \n",
       "\n",
       "       ticket=250647  ticket=27849  ticket=28220 L32 10s  \\\n",
       "count    1313.000000   1313.000000           1313.000000   \n",
       "mean        0.000762      0.000762              0.002285   \n",
       "std         0.027597      0.027597              0.047764   \n",
       "min         0.000000      0.000000              0.000000   \n",
       "25%         0.000000      0.000000              0.000000   \n",
       "50%         0.000000      0.000000              0.000000   \n",
       "75%         0.000000      0.000000              0.000000   \n",
       "max         1.000000      1.000000              1.000000   \n",
       "\n",
       "       ticket=34218 L10 10s  ticket=36973 L83 9s 6d  ticket=392091  \\\n",
       "count           1313.000000             1313.000000    1313.000000   \n",
       "mean               0.000762                0.001523       0.001523   \n",
       "std                0.027597                0.039014       0.039014   \n",
       "min                0.000000                0.000000       0.000000   \n",
       "25%                0.000000                0.000000       0.000000   \n",
       "50%                0.000000                0.000000       0.000000   \n",
       "75%                0.000000                0.000000       0.000000   \n",
       "max                1.000000                1.000000       1.000000   \n",
       "\n",
       "       ticket=7076  ticket=L15 1s  \n",
       "count  1313.000000    1313.000000  \n",
       "mean      0.000762       0.000762  \n",
       "std       0.027597       0.027597  \n",
       "min       0.000000       0.000000  \n",
       "25%       0.000000       0.000000  \n",
       "50%       0.000000       0.000000  \n",
       "75%       0.000000       0.000000  \n",
       "max       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 580 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to deal with missing values, since DecisionTreeClassifier we plan\n",
    "to use does not admit them on input. Pandas allow us to replace them with a fixed\n",
    "value using the fillna method. We will use the mean age for the age feature, and 0\n",
    "for the remaining missing attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = titanic['age'].mean()\n",
    "titanic['age'].fillna(mean, inplace=True)\n",
    "titanic.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of our features (except for Name) are in a suitable format. We are ready to\n",
    "build the test and training sets, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "titanic_target = titanic['survived']\n",
    "titanic_data = titanic.drop(['name', 'row.names', 'survived'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_data, titanic_target, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to simply drop the name attribute, since we do not expect it to be\n",
    "informative about the survival status (we have one different value for each instance,\n",
    "so we can generalize over it). We also specified the survived feature as the target\n",
    "class, and consequently eliminated it from the training vector.\n",
    "Let's see how a decision tree works with the current feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dt = dt.fit(X_train, y_train)\n",
    "from sklearn import metrics\n",
    "y_pred = dt.predict(X_test)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, when training our decision tree, we used every available feature in our\n",
    "learning dataset. This seems perfectly reasonable, since we want to use as much\n",
    "information as there is available to build our model. There are, however, two main\n",
    "reasons why we would want to __restrict the number of features used__:\n",
    " - Firstly, for some methods, especially those (such as decision trees) that\n",
    "reduce the number of instances used to refine the model at each step, it is\n",
    "possible that irrelevant features could suggest correlations between features\n",
    "and target classes that arise just by chance and do not correctly model the\n",
    "problem. This aspect is also related to __overfitting__; having certain over-specific\n",
    "features may lead to poor generalization. Besides, some features may be\n",
    "highly correlated, and will simply add redundant information.\n",
    " - The second reason is a real-world one. A large number of features could\n",
    "greatly increase the __computation time__ without a corresponding classifier\n",
    "improvement. This is of particular importance when working with Big Data,\n",
    "where the number of instances and features could easily grow to several\n",
    "thousand or more. Also, in relation to the curse of dimensionality, learning\n",
    "a generalizable model from a dataset with too many features relative to the\n",
    "number of instances can be difficult.\n",
    "\n",
    "As a result, working with a smaller feature set may lead to better results. So we want\n",
    "to find some way to algorithmically find the best features. This task is called feature\n",
    "selection and is a crucial step when we aim to get decent results with machine\n",
    "learning algorithms. If we have poor features, our algorithm will return poor results\n",
    "no matter how sophisticated our machine learning algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, for example, our very simple Titanic example. We started with just 11\n",
    "features, but after 1-of-K encoding they grew to 581."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names  survived                                             name  \\\n",
      "0          1         1                     Allen, Miss Elisabeth Walton   \n",
      "1          2         0                      Allison, Miss Helen Loraine   \n",
      "2          3         0              Allison, Mr Hudson Joshua Creighton   \n",
      "3          4         0  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)   \n",
      "4          5         1                    Allison, Master Hudson Trevor   \n",
      "\n",
      "       age  embarked  embarked=Cherbourg  embarked=Queenstown  \\\n",
      "0  29.0000         0                   0                    0   \n",
      "1   2.0000         0                   0                    0   \n",
      "2  30.0000         0                   0                    0   \n",
      "3  25.0000         0                   0                    0   \n",
      "4   0.9167         0                   0                    0   \n",
      "\n",
      "   embarked=Southampton  pclass=1st  pclass=2nd      ...        \\\n",
      "0                     1           1           0      ...         \n",
      "1                     1           1           0      ...         \n",
      "2                     1           1           0      ...         \n",
      "3                     1           1           0      ...         \n",
      "4                     1           1           0      ...         \n",
      "\n",
      "   ticket=248744 L13  ticket=248749 L13  ticket=250647  ticket=27849  \\\n",
      "0                  0                  0              0             0   \n",
      "1                  0                  0              0             0   \n",
      "2                  0                  0              0             0   \n",
      "3                  0                  0              0             0   \n",
      "4                  0                  0              0             0   \n",
      "\n",
      "   ticket=28220 L32 10s  ticket=34218 L10 10s  ticket=36973 L83 9s 6d  \\\n",
      "0                     0                     0                       0   \n",
      "1                     0                     0                       0   \n",
      "2                     0                     0                       0   \n",
      "3                     0                     0                       0   \n",
      "4                     0                     0                       0   \n",
      "\n",
      "   ticket=392091  ticket=7076  ticket=L15 1s  \n",
      "0              0            0              0  \n",
      "1              0            0              0  \n",
      "2              0            0              0  \n",
      "3              0            0              0  \n",
      "4              0            0              0  \n",
      "\n",
      "[5 rows x 581 columns]\n"
     ]
    }
   ],
   "source": [
    "print (titanic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not pose an important computational problem, but consider what could\n",
    "happen if, as previously demonstrated, we represent each document in a dataset as\n",
    "the number of occurrences of each possible word. Another problem is that decision\n",
    "trees suffer from overfitting. If branching is based on a very small number of\n",
    "instances, the prediction power of the built model will decrease on future data. One\n",
    "solution to this is to adjust model parameters (such as the maximum tree depth or\n",
    "the minimum required number of instances at a leaf node). In this example, however,\n",
    "we will take a different approach: we will try to limit the features to the most\n",
    "relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we mean by relevant? This is an important question. A general approach\n",
    "is to find the smallest set of features that correctly characterize the training data. If\n",
    "a feature always coincides with the target class (that is, it is a perfect predictor), it is\n",
    "enough to characterize the data. On the other hand, if a feature always has the same\n",
    "value, its prediction power will be very low.\n",
    "\n",
    "The general approach in feature selection is to get some kind of evaluation function\n",
    "that, when given a potential feature, returns a score of how useful the feature is,\n",
    "and then keeps the features with the highest scores. These methods may have the\n",
    "disadvantage of not detecting correlations between features. Other methods may\n",
    "be more brute force: try all possible subsets of the original feature list, train the\n",
    "algorithm on each combination, and keep the combination that gets the best results.\n",
    "\n",
    "As an evaluation method, we can, for instance, use a statistical test that measures\n",
    "how probable it is that two random variables (say, a given feature and the target\n",
    "class) are independent; that is, there is no correlation between them.\n",
    "\n",
    "Scikit-learn provides several methods in the __feature_selection module__. We will\n",
    "use the __SelectPercentile__ method that, when given a statistical test, selects a userspecified\n",
    "percentile of features with the highest scoring. __The most popular statistical\n",
    "test is the χ² (chi-squared) statistic__. Let's see how it works for our Titanic example; we\n",
    "will use it to select 20 percent of the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=20)\n",
    "X_train_fs = fs.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_train_fs array now has the statistically more important features. We can\n",
    "now train our decision tree on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.848 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred_fs)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy on the training set improved half a point after feature\n",
    "selection on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to find the optimal number of features? If by optimal we mean with\n",
    "the best performance on the training set, it is actually possible; we can simply use\n",
    "a brute-force approach and try with different numbers of features while measuring\n",
    "their performance on the training set using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.83332302618\n",
      "6 0.87804576376\n",
      "11 0.872984951556\n",
      "16 0.872974644403\n",
      "21 0.870933828077\n",
      "26 0.869913419913\n",
      "31 0.86381158524\n",
      "36 0.866883116883\n",
      "41 0.873984745413\n",
      "46 0.877045969903\n",
      "51 0.86990311276\n",
      "56 0.867862296434\n",
      "61 0.870933828077\n",
      "66 0.872974644403\n",
      "71 0.866852195424\n",
      "76 0.865842094414\n",
      "81 0.868934240363\n",
      "86 0.863821892393\n",
      "91 0.86687280973\n",
      "96 0.866862502577\n",
      "Optimal number of features:6 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1952281db00>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195221ee5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "percentile_step = 5 # try 1, 2, 5, 10, etc.\n",
    "\n",
    "percentiles = range(1, 100, percentile_step)\n",
    "results = []\n",
    "for i in range(1,100,percentile_step):\n",
    "    fs = feature_selection.SelectPercentile(\n",
    "        feature_selection.chi2, percentile=i)\n",
    "    X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "    scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "    print (i, scores.mean())\n",
    "    results = np.append(results, scores.mean())\n",
    "\n",
    "optimal_percentil = np.where(results == results.max())[0]\n",
    "print (\"Optimal number of features:{0}\".format(percentiles[optimal_percentil[0]]), \"\\n\")\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "import pylab as pl\n",
    "pl.figure()\n",
    "pl.xlabel(\"Number of features selected\")\n",
    "pl.ylabel(\"Cross-validation accuracy)\")\n",
    "pl.plot(percentiles, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that accuracy quickly improves when we start adding features, remaining\n",
    "stable after the percentile of features turns about 10. In fact, the best accuracy is\n",
    "achieved when using 64 of the original 581 features (at the 11 percent percentile).\n",
    "Let's see if this actually improved performance on the testing set.\n",
    "\n",
    "Note: results are diffrent from statement above. Probably Python 2 and Python 3 calculate _little difffrent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=percentiles[optimal_percentil[0]])\n",
    "X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred_fs)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance improved slightly, again. Compared with our initial performance,\n",
    "we have finally improved by almost one accuracy point using only 11 percent of\n",
    "the features.\n",
    "\n",
    "The reader may have noted that while creating our classifier, we used the default\n",
    "parameters, except for the splitting criterion, where we have used entropy. Can we\n",
    "improve our model using different parameters? This task is called model selection,\n",
    "and we will address it in detail in the next section using a different learning example.\n",
    "\n",
    "For now, let's just test if the alternative method (gini) would result in better\n",
    "performance for our example. To do this, we will again use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy criterion accuracy on cv: 0.877\n",
      "Gini criterion accuracy on cv: 0.880\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print (\"Entropy criterion accuracy on cv: {0:.3f}\".format(scores.mean()))\n",
    "dt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print (\"Gini criterion accuracy on cv: {0:.3f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini criterion performs better on our training set. How about its performance on\n",
    "the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy: {0:.3f}\".format(metrics.accuracy_score(y_test,y_pred_fs)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that performance improvement on the training set did not hold for the\n",
    "evaluation set. This is always possible. In fact, performance could have decreased\n",
    "(recall overfitting). Our model is still the best. If we changed our model to use the one\n",
    "with the best performance in the testing set, we can never measure its performance,\n",
    "since the testing dataset could not be considered \"unseen data\" anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
