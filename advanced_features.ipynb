{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Python version: 3.5.2\n",
      "IPython version: 4.0.1\n",
      "numpy version: 1.13.1\n",
      "scikit-learn version: 0.18.2\n",
      "matplotlib version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import platform\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters we have studied several algorithms for very different tasks,\n",
    "from classification and regression to clustering and dimensionality reduction. We\n",
    "showed how we can apply these algorithms to predict results when faced with new\n",
    "data. That is what machine learning is all about. In this last chapter, we want to show\n",
    "some important concepts and methods you should take into account if you want to\n",
    "do real-world machine learning.\n",
    "- In real-world problems, usually data is not already expressed by attribute/\n",
    "float value pairs, but through more complex structures or is not structured at\n",
    "all. We will learn __feature extraction__ techniques that will allow us to extract\n",
    "scikit-learn features from data.\n",
    "- From the initial set of available features, not all of them will be useful\n",
    "for our algorithms to learn from; in fact, some of them may degrade our\n",
    "performance. We will address the problem of selecting the most adequate\n",
    "feature set, a process known as __feature selection__.\n",
    "- Finally, as we have seen in the examples in this book, many of the machine\n",
    "learning algorithms have parameters that must be set in order to use them.\n",
    "To do that, we will review __model selection__ techniques; that is, methods to\n",
    "select the most promising hyperparameters to our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...the source data does not usually come in this format. We have to\n",
    "extract what we think are potentially useful features and convert them to our learning\n",
    "format. This process is called feature extraction or feature engineering, and it is an\n",
    "often underestimated but very important and time-consuming phase in most realworld\n",
    "machine learning tasks. We can identify two different steps in this task:\n",
    " - __Obtain features__: This step involves processing the source data and extracting\n",
    "the learning instances, usually in the form of feature/value pairs where\n",
    "the value can be an integer or float value, a string, a categorical value, and\n",
    "so on. The method used for extraction depends heavily on how the data\n",
    "is presented. For example, we can have a set of pictures and generate an\n",
    "integer-valued feature for each pixel, indicating its color level, as we did\n",
    "in the face recognition example in Chapter 2, Supervised Learning. Since this\n",
    "is a very task-dependent job, we will not delve into details and assume we\n",
    "already have this setting for our examples.\n",
    " - __Convert features__: Most scikit-learn algorithms assume as an input a set of\n",
    "instances represented as a list of float-valued features. How to get these\n",
    "features will be the main subject of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, as we did in Chapter 2, Supervised Learning, build ad hoc procedures to\n",
    "convert the source data. There are, however, tools that can help us to obtain a\n",
    "suitable representation. The Python package __pandas__ (http://pandas.pydata.\n",
    "org/), for example, provides data structures and tools for data analysis. It aims to\n",
    "provide similar features to those of R, the popular language and environment for\n",
    "statistical computing. We will use pandas to import the Titanic data we presented in\n",
    "Chapter 2, Supervised Learning, and convert them to the scikit-learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names pclass  survived  \\\n",
      "0          1    1st         1   \n",
      "1          2    1st         0   \n",
      "2          3    1st         0   \n",
      "3          4    1st         0   \n",
      "4          5    1st         1   \n",
      "\n",
      "                                              name      age     embarked  \\\n",
      "0                     Allen, Miss Elisabeth Walton  29.0000  Southampton   \n",
      "1                      Allison, Miss Helen Loraine   2.0000  Southampton   \n",
      "2              Allison, Mr Hudson Joshua Creighton  30.0000  Southampton   \n",
      "3  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)  25.0000  Southampton   \n",
      "4                    Allison, Master Hudson Trevor   0.9167  Southampton   \n",
      "\n",
      "                         home.dest room      ticket   boat     sex  \n",
      "0                     St Louis, MO  B-5  24160 L221      2  female  \n",
      "1  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "2  Montreal, PQ / Chesterville, ON  C26         NaN  (135)    male  \n",
      "3  Montreal, PQ / Chesterville, ON  C26         NaN    NaN  female  \n",
      "4  Montreal, PQ / Chesterville, ON  C22         NaN     11    male  \n"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "print (titanic[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pclass  survived      age     embarked   boat     sex\n",
      "0    1st         1  29.0000  Southampton      2  female\n",
      "1    1st         0   2.0000  Southampton    NaN  female\n",
      "2    1st         0  30.0000  Southampton  (135)    male\n",
      "3    1st         0  25.0000  Southampton    NaN  female\n",
      "4    1st         1   0.9167  Southampton     11    male\n"
     ]
    }
   ],
   "source": [
    "print (titanic.head()[['pclass', 'survived', 'age', 'embarked',\n",
    "'boat', 'sex']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty we have now is that scikit-learn methods expect real numbers\n",
    "as feature values. In Chapter 2, Supervised Learning, we used the LabelEncoder and\n",
    "OneHotEncoder preprocessing methods to manually convert certain categorical\n",
    "features into 1-of-K values (generating a new feature for each possible value; valued\n",
    "1 if the original feature had the corresponding value and 0 otherwise). This time, we\n",
    "will use a similar scikit-learn method, __DictVectorizer__, which automatically builds\n",
    "these features from the different original feature values. Moreover, we will program\n",
    "a method to encode a set of columns in a unique step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "def one_hot_dataframe(data, cols, replace=False):\n",
    "    vec = feature_extraction.DictVectorizer()\n",
    "    mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
    "    vecData = pd.DataFrame(vec.fit_transform( data[cols].apply(mkdict, axis=1)).toarray())\n",
    "    vecData.columns = vec.get_feature_names()\n",
    "    vecData.index = data.index\n",
    "    if replace:\n",
    "        data = data.drop(cols, axis=1)\n",
    "        data = data.join(vecData)\n",
    "    return (data, vecData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one_hot_dataframe method (based on the script at https://gist.github.\n",
    "com/kljensen/5452382) takes a pandas DataFrame data structure and a list of\n",
    "columns and encodes each column into the necessary 1-of-K features. If the replace\n",
    "parameter is True, it will also substitute the original column with the new set. Let's\n",
    "see it applied to the categorical pclass, embarked, and sex features (titanic_n only\n",
    "contains the previously created columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked=Cherbourg</th>\n",
       "      <th>embarked=Queenstown</th>\n",
       "      <th>embarked=Southampton</th>\n",
       "      <th>pclass=1st</th>\n",
       "      <th>pclass=2nd</th>\n",
       "      <th>pclass=3rd</th>\n",
       "      <th>sex=female</th>\n",
       "      <th>sex=male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>821</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.341965</td>\n",
       "      <td>31.194181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154608</td>\n",
       "      <td>0.034273</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.245240</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>0.352628</td>\n",
       "      <td>0.647372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>379.174762</td>\n",
       "      <td>0.474549</td>\n",
       "      <td>14.747525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361668</td>\n",
       "      <td>0.181998</td>\n",
       "      <td>0.496128</td>\n",
       "      <td>0.430393</td>\n",
       "      <td>0.409760</td>\n",
       "      <td>0.498464</td>\n",
       "      <td>0.477970</td>\n",
       "      <td>0.477970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>329.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         row.names     survived         age  embarked  embarked=Cherbourg  \\\n",
       "count  1313.000000  1313.000000  633.000000       821         1313.000000   \n",
       "mean    657.000000     0.341965   31.194181         0            0.154608   \n",
       "std     379.174762     0.474549   14.747525         0            0.361668   \n",
       "min       1.000000     0.000000    0.166700         0            0.000000   \n",
       "25%     329.000000     0.000000   21.000000         0            0.000000   \n",
       "50%     657.000000     0.000000   30.000000         0            0.000000   \n",
       "75%     985.000000     1.000000   41.000000         0            0.000000   \n",
       "max    1313.000000     1.000000   71.000000         0            1.000000   \n",
       "\n",
       "       embarked=Queenstown  embarked=Southampton   pclass=1st   pclass=2nd  \\\n",
       "count          1313.000000           1313.000000  1313.000000  1313.000000   \n",
       "mean              0.034273              0.436405     0.245240     0.213252   \n",
       "std               0.181998              0.496128     0.430393     0.409760   \n",
       "min               0.000000              0.000000     0.000000     0.000000   \n",
       "25%               0.000000              0.000000     0.000000     0.000000   \n",
       "50%               0.000000              0.000000     0.000000     0.000000   \n",
       "75%               0.000000              1.000000     0.000000     0.000000   \n",
       "max               1.000000              1.000000     1.000000     1.000000   \n",
       "\n",
       "        pclass=3rd   sex=female     sex=male  \n",
       "count  1313.000000  1313.000000  1313.000000  \n",
       "mean      0.541508     0.352628     0.647372  \n",
       "std       0.498464     0.477970     0.477970  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       1.000000     0.000000     1.000000  \n",
       "75%       1.000000     1.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic,titanic_n = one_hot_dataframe(titanic, ['pclass', 'embarked', 'sex'], replace=True)\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pclass attribute has been converted to three pclass=1st, pclass=2nd,\n",
    "pclass=3rd features, and similarly for the other two features. Note that the\n",
    "embarked feature has not disappeared, This is due to the fact that the original\n",
    "embarked attribute included NaN values, indicating a missing value; in those cases,\n",
    "every feature based on embarked will be valued 0, but the original feature whose\n",
    "value is NaN remains, indicating the feature is missing for certain instances. Next, we\n",
    "encode the remaining categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic, titanic_n = one_hot_dataframe(titanic, ['home.dest', 'room', 'ticket', 'boat'], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked=Cherbourg</th>\n",
       "      <th>embarked=Queenstown</th>\n",
       "      <th>embarked=Southampton</th>\n",
       "      <th>pclass=1st</th>\n",
       "      <th>pclass=2nd</th>\n",
       "      <th>pclass=3rd</th>\n",
       "      <th>...</th>\n",
       "      <th>ticket=248744 L13</th>\n",
       "      <th>ticket=248749 L13</th>\n",
       "      <th>ticket=250647</th>\n",
       "      <th>ticket=27849</th>\n",
       "      <th>ticket=28220 L32 10s</th>\n",
       "      <th>ticket=34218 L10 10s</th>\n",
       "      <th>ticket=36973 L83 9s 6d</th>\n",
       "      <th>ticket=392091</th>\n",
       "      <th>ticket=7076</th>\n",
       "      <th>ticket=L15 1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>821</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.341965</td>\n",
       "      <td>31.194181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154608</td>\n",
       "      <td>0.034273</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.245240</td>\n",
       "      <td>0.213252</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>379.174762</td>\n",
       "      <td>0.474549</td>\n",
       "      <td>14.747525</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361668</td>\n",
       "      <td>0.181998</td>\n",
       "      <td>0.496128</td>\n",
       "      <td>0.430393</td>\n",
       "      <td>0.409760</td>\n",
       "      <td>0.498464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.047764</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.027597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>329.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>657.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 580 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         row.names     survived         age  embarked  embarked=Cherbourg  \\\n",
       "count  1313.000000  1313.000000  633.000000       821         1313.000000   \n",
       "mean    657.000000     0.341965   31.194181         0            0.154608   \n",
       "std     379.174762     0.474549   14.747525         0            0.361668   \n",
       "min       1.000000     0.000000    0.166700         0            0.000000   \n",
       "25%     329.000000     0.000000   21.000000         0            0.000000   \n",
       "50%     657.000000     0.000000   30.000000         0            0.000000   \n",
       "75%     985.000000     1.000000   41.000000         0            0.000000   \n",
       "max    1313.000000     1.000000   71.000000         0            1.000000   \n",
       "\n",
       "       embarked=Queenstown  embarked=Southampton   pclass=1st   pclass=2nd  \\\n",
       "count          1313.000000           1313.000000  1313.000000  1313.000000   \n",
       "mean              0.034273              0.436405     0.245240     0.213252   \n",
       "std               0.181998              0.496128     0.430393     0.409760   \n",
       "min               0.000000              0.000000     0.000000     0.000000   \n",
       "25%               0.000000              0.000000     0.000000     0.000000   \n",
       "50%               0.000000              0.000000     0.000000     0.000000   \n",
       "75%               0.000000              1.000000     0.000000     0.000000   \n",
       "max               1.000000              1.000000     1.000000     1.000000   \n",
       "\n",
       "        pclass=3rd      ...        ticket=248744 L13  ticket=248749 L13  \\\n",
       "count  1313.000000      ...              1313.000000        1313.000000   \n",
       "mean      0.541508      ...                 0.000762           0.000762   \n",
       "std       0.498464      ...                 0.027597           0.027597   \n",
       "min       0.000000      ...                 0.000000           0.000000   \n",
       "25%       0.000000      ...                 0.000000           0.000000   \n",
       "50%       1.000000      ...                 0.000000           0.000000   \n",
       "75%       1.000000      ...                 0.000000           0.000000   \n",
       "max       1.000000      ...                 1.000000           1.000000   \n",
       "\n",
       "       ticket=250647  ticket=27849  ticket=28220 L32 10s  \\\n",
       "count    1313.000000   1313.000000           1313.000000   \n",
       "mean        0.000762      0.000762              0.002285   \n",
       "std         0.027597      0.027597              0.047764   \n",
       "min         0.000000      0.000000              0.000000   \n",
       "25%         0.000000      0.000000              0.000000   \n",
       "50%         0.000000      0.000000              0.000000   \n",
       "75%         0.000000      0.000000              0.000000   \n",
       "max         1.000000      1.000000              1.000000   \n",
       "\n",
       "       ticket=34218 L10 10s  ticket=36973 L83 9s 6d  ticket=392091  \\\n",
       "count           1313.000000             1313.000000    1313.000000   \n",
       "mean               0.000762                0.001523       0.001523   \n",
       "std                0.027597                0.039014       0.039014   \n",
       "min                0.000000                0.000000       0.000000   \n",
       "25%                0.000000                0.000000       0.000000   \n",
       "50%                0.000000                0.000000       0.000000   \n",
       "75%                0.000000                0.000000       0.000000   \n",
       "max                1.000000                1.000000       1.000000   \n",
       "\n",
       "       ticket=7076  ticket=L15 1s  \n",
       "count  1313.000000    1313.000000  \n",
       "mean      0.000762       0.000762  \n",
       "std       0.027597       0.027597  \n",
       "min       0.000000       0.000000  \n",
       "25%       0.000000       0.000000  \n",
       "50%       0.000000       0.000000  \n",
       "75%       0.000000       0.000000  \n",
       "max       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 580 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to deal with missing values, since DecisionTreeClassifier we plan\n",
    "to use does not admit them on input. Pandas allow us to replace them with a fixed\n",
    "value using the fillna method. We will use the mean age for the age feature, and 0\n",
    "for the remaining missing attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = titanic['age'].mean()\n",
    "titanic['age'].fillna(mean, inplace=True)\n",
    "titanic.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of our features (except for Name) are in a suitable format. We are ready to\n",
    "build the test and training sets, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "titanic_target = titanic['survived']\n",
    "titanic_data = titanic.drop(['name', 'row.names', 'survived'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_data, titanic_target, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to simply drop the name attribute, since we do not expect it to be\n",
    "informative about the survival status (we have one different value for each instance,\n",
    "so we can generalize over it). We also specified the survived feature as the target\n",
    "class, and consequently eliminated it from the training vector.\n",
    "Let's see how a decision tree works with the current feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dt = dt.fit(X_train, y_train)\n",
    "from sklearn import metrics\n",
    "y_pred = dt.predict(X_test)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, when training our decision tree, we used every available feature in our\n",
    "learning dataset. This seems perfectly reasonable, since we want to use as much\n",
    "information as there is available to build our model. There are, however, two main\n",
    "reasons why we would want to __restrict the number of features used__:\n",
    " - Firstly, for some methods, especially those (such as decision trees) that\n",
    "reduce the number of instances used to refine the model at each step, it is\n",
    "possible that irrelevant features could suggest correlations between features\n",
    "and target classes that arise just by chance and do not correctly model the\n",
    "problem. This aspect is also related to __overfitting__; having certain over-specific\n",
    "features may lead to poor generalization. Besides, some features may be\n",
    "highly correlated, and will simply add redundant information.\n",
    " - The second reason is a real-world one. A large number of features could\n",
    "greatly increase the __computation time__ without a corresponding classifier\n",
    "improvement. This is of particular importance when working with Big Data,\n",
    "where the number of instances and features could easily grow to several\n",
    "thousand or more. Also, in relation to the curse of dimensionality, learning\n",
    "a generalizable model from a dataset with too many features relative to the\n",
    "number of instances can be difficult.\n",
    "\n",
    "As a result, working with a smaller feature set may lead to better results. So we want\n",
    "to find some way to algorithmically find the best features. This task is called feature\n",
    "selection and is a crucial step when we aim to get decent results with machine\n",
    "learning algorithms. If we have poor features, our algorithm will return poor results\n",
    "no matter how sophisticated our machine learning algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider, for example, our very simple Titanic example. We started with just 11\n",
    "features, but after 1-of-K encoding they grew to 581."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row.names  survived                                             name  \\\n",
      "0          1         1                     Allen, Miss Elisabeth Walton   \n",
      "1          2         0                      Allison, Miss Helen Loraine   \n",
      "2          3         0              Allison, Mr Hudson Joshua Creighton   \n",
      "3          4         0  Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)   \n",
      "4          5         1                    Allison, Master Hudson Trevor   \n",
      "\n",
      "       age  embarked  embarked=Cherbourg  embarked=Queenstown  \\\n",
      "0  29.0000         0                   0                    0   \n",
      "1   2.0000         0                   0                    0   \n",
      "2  30.0000         0                   0                    0   \n",
      "3  25.0000         0                   0                    0   \n",
      "4   0.9167         0                   0                    0   \n",
      "\n",
      "   embarked=Southampton  pclass=1st  pclass=2nd      ...        \\\n",
      "0                     1           1           0      ...         \n",
      "1                     1           1           0      ...         \n",
      "2                     1           1           0      ...         \n",
      "3                     1           1           0      ...         \n",
      "4                     1           1           0      ...         \n",
      "\n",
      "   ticket=248744 L13  ticket=248749 L13  ticket=250647  ticket=27849  \\\n",
      "0                  0                  0              0             0   \n",
      "1                  0                  0              0             0   \n",
      "2                  0                  0              0             0   \n",
      "3                  0                  0              0             0   \n",
      "4                  0                  0              0             0   \n",
      "\n",
      "   ticket=28220 L32 10s  ticket=34218 L10 10s  ticket=36973 L83 9s 6d  \\\n",
      "0                     0                     0                       0   \n",
      "1                     0                     0                       0   \n",
      "2                     0                     0                       0   \n",
      "3                     0                     0                       0   \n",
      "4                     0                     0                       0   \n",
      "\n",
      "   ticket=392091  ticket=7076  ticket=L15 1s  \n",
      "0              0            0              0  \n",
      "1              0            0              0  \n",
      "2              0            0              0  \n",
      "3              0            0              0  \n",
      "4              0            0              0  \n",
      "\n",
      "[5 rows x 581 columns]\n"
     ]
    }
   ],
   "source": [
    "print (titanic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not pose an important computational problem, but consider what could\n",
    "happen if, as previously demonstrated, we represent each document in a dataset as\n",
    "the number of occurrences of each possible word. Another problem is that decision\n",
    "trees suffer from overfitting. If branching is based on a very small number of\n",
    "instances, the prediction power of the built model will decrease on future data. One\n",
    "solution to this is to adjust model parameters (such as the maximum tree depth or\n",
    "the minimum required number of instances at a leaf node). In this example, however,\n",
    "we will take a different approach: we will try to limit the features to the most\n",
    "relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we mean by relevant? This is an important question. A general approach\n",
    "is to find the smallest set of features that correctly characterize the training data. If\n",
    "a feature always coincides with the target class (that is, it is a perfect predictor), it is\n",
    "enough to characterize the data. On the other hand, if a feature always has the same\n",
    "value, its prediction power will be very low.\n",
    "\n",
    "The general approach in feature selection is to get some kind of evaluation function\n",
    "that, when given a potential feature, returns a score of how useful the feature is,\n",
    "and then keeps the features with the highest scores. These methods may have the\n",
    "disadvantage of not detecting correlations between features. Other methods may\n",
    "be more brute force: try all possible subsets of the original feature list, train the\n",
    "algorithm on each combination, and keep the combination that gets the best results.\n",
    "\n",
    "As an evaluation method, we can, for instance, use a statistical test that measures\n",
    "how probable it is that two random variables (say, a given feature and the target\n",
    "class) are independent; that is, there is no correlation between them.\n",
    "\n",
    "Scikit-learn provides several methods in the __feature_selection module__. We will\n",
    "use the __SelectPercentile__ method that, when given a statistical test, selects a userspecified\n",
    "percentile of features with the highest scoring. __The most popular statistical\n",
    "test is the χ² (chi-squared) statistic__. Let's see how it works for our Titanic example; we\n",
    "will use it to select 20 percent of the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=20)\n",
    "X_train_fs = fs.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_train_fs array now has the statistically more important features. We can\n",
    "now train our decision tree on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.848 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred_fs)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy on the training set improved half a point after feature\n",
    "selection on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it possible to find the optimal number of features? If by optimal we mean with\n",
    "the best performance on the training set, it is actually possible; we can simply use\n",
    "a brute-force approach and try with different numbers of features while measuring\n",
    "their performance on the training set using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.83332302618\n",
      "6 0.87804576376\n",
      "11 0.872984951556\n",
      "16 0.872974644403\n",
      "21 0.870933828077\n",
      "26 0.869913419913\n",
      "31 0.86381158524\n",
      "36 0.866883116883\n",
      "41 0.873984745413\n",
      "46 0.877045969903\n",
      "51 0.86990311276\n",
      "56 0.867862296434\n",
      "61 0.870933828077\n",
      "66 0.872974644403\n",
      "71 0.866852195424\n",
      "76 0.865842094414\n",
      "81 0.868934240363\n",
      "86 0.863821892393\n",
      "91 0.86687280973\n",
      "96 0.866862502577\n",
      "Optimal number of features:6 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1952281db00>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm8nPP5//HXlZBGNmKt0ETtxL6kthIRW33RH1WU1laU\nIJZaq/ZWLUGRKq2SViq2qhQlEQ5VWxB7JLGTqIREJLIn1++P6z4yjpNz7jNz3zNz5ryfj8c8zsw9\n99z3dSYnc83n/nw+18fcHRERkZZqV+kARESkdVICERGRoiiBiIhIUZRARESkKEogIiJSFCUQEREp\nSu4JxMz2MLM3zWy8mZ3ZyPPdzGy4mb1kZq+a2eEFz51iZq+Z2StmNtTMOuQdr4iIpGN5zgMxs3bA\neGAXYBIwGjjI3d8s2OdsoJu7n21mKwLjgFWAlYEngfXdfZ6Z3QE84O5/zS1gERFJLe8WSB9ggru/\n7+7zgWHAvg32caBrcr8r8Jm7L0getwc6m9lSQCciCYmISBXIO4GsBnxY8PijZFuh64ENzWwS8DIw\nEMDdJwGDgA+AicDn7v5IzvGKiEhK1dCJvjswxt17AJsDg82si5ktR7RWegE9gC5m9pMKxikiIgWW\nyvn4E4GeBY9XT7YVOgK4FMDd3zazd4H1gTWAd9x9KoCZ/QPYDvh7w5OYmQp6iYi0kLtbKa/PuwUy\nGljbzHolI6gOAoY32Od9oD+Ama0CrAu8Q1y62sbMOpqZER3xY5d0InfXzZ3zzz+/4jFUw03vg94L\nvRdN37KQawvE3Rea2QnACCJZ3ezuY83s2HjabwIuAW41s1eSl53h0ep4zszuBsYA85OfN+UZr4iI\npJf3JSzc/SFgvQbbbiy4/zHRD9LYay8ELsw1QBERKUo1dKJLhvr27VvpEKqC3ofF9F4spvciW7lO\nJCwXM/Na+D1ERMrFzPAq70QXEZEapQTSwMyZlY5ARKR1UAJpYJttYPDgSkchIlL91AfSwLLLQvv2\n8OijsNlmmRxSRKTqqA8kY3PnwuzZcN118OMfw4wZlY5IRKR6KYEUmDIFVlwRDjkEdtwRjjsOaqCB\nJiKSCyWQAlOmwMorx/1rr4UxY+DWWysaklSB//0vvkxMm1bpSESqixJIgcmTYaWV4n6nTnDnnXDG\nGfDGG5WNSyrn+eehTx+oq4Mrrqh0NCLVRQmkQGELBKB3b/jd76I/ZNasysUllfH3v8Oee8LVV8PD\nD8ONN0ZrRESCEkiBwhZIvSOPhE03hZNPrkxMUn4LF8JZZ8GvfgWjRsH++0PPnvCzn8FvflPp6ESq\nhxJIgSlTvplAzOCPf4xLGLffXpGwpIymT4d99oFnn4XRo2GTTRY/d/bZ0Sp5772KhSdSVZRACkye\n/PVLWPW6doU77oCTToK33ip/XFIe48fHRNI11oARI2JEXqGVV4bjj4cLVR9aBFAC+ZrGWiD1Nt8c\nzj8fDjww5otIbXn4YdhhBzjllKhEsPTSje/3y1/CAw9oYIUIKIF8zZJaIPUGDIBevWJkltQGdxg0\nCI44Au65B445pun9l102ksh555UnPpFqlvuCUq1JUy0QiP6Qm2+O1sjOO8MPf1i+2CR7c+ZEwnj1\nVXjmmegoT+OEE2CddWKI71Zb5RujSDVTC6RAcy0QgO7dYdiw+OB5//3yxCXZmzQJdtopLkc++WT6\n5AExR+jcc2OUlkhbpgSSmDMH5s2Dbt2a33ebbeD00+Hgg2H+/Pxjk2w9+2xMDtx33/gy0Llzy49x\n1FEwYUKMzhNpq5RAEvWXryxlbcrTToPlloNf/zrfuCRbf/0r7L033HADnHNO+n/vhjp0gIsuimOo\nXpq0VUogicYmETalXTsYMgRuuw0eeii/uCQbCxZE0r/oomg17L136cc8+GD44osYlSXSFimBJBqW\nMUljpZVg6NAYwTNpUj5xSemmTYO99oJXXoHnnoMNN8zmuO3bwyWXRF/IokXZHFOkNVECSbS0BVJv\np52iUushh0QJDKku7lGKZJ114N//huWXz/b4++4LHTvGRFORtkYJJFFMC6Ter34V19IvuSTbmKR0\nI0bAxx/DNdfAUjkMWjeD3/425oVoQIW0NUogiWJbIBCXMm67bXHNLKkOixZFJ/fFF+eTPOrtsktM\nML3llvzOIVKNlEASpbRAAHr0iMWnDj00jiWVd/fd0ULYf//8z/Wb30QH/ezZ+Z9LpFpoJnqilBZI\nvd13jwSy554xzwDiA6z+1tTjhs+ttx4cdtiSazJJ0xYsiCHW119f/FDdlvje92JW+g03wKmn5n++\nSvnkk6hKPWCA/jZFCeQrzZUxSevii2H99eHLL+Ox++Jb4ePmnrvzTrj88ri+vv/+5fkQrCW33gqr\nrQb9+5fvnJdcEpezfv7zdBNSW5OZM+HKK+G666Iaw/z5MZlW2jbzGpgFZWZe6u/x3e/CI4/AWmtl\nFFQGRo6Mwo0dO0Yy+f73Kx1R6zBnToy6uuuuqBpQTj/9Kay9dlRurgXz58Of/xxfjPr1iyS5YEG8\nry++2LISMFJdzAx3L+mrqRJIokuXmMtRbd8cFy2KRYzOPTcWN/rd77Kbx1CrrroKnngC/vnP8p/7\nnXdg661h3LhvrifSmrjDvffGIlo9e8Jll8EWWyx+/sIL4aWXYh9pnZRAEqUmkFmzYn7A7NnVe6lo\nzhz4wx8igey7b/wH7tGj0lFVny++iNbHqFGw0UaVieH446Pg4pVXVub8pXryyWj5zpoVLd/ddvvm\nPnPmwMYbx3rx//d/5Y9RSpdFAtEoLFpeB6sSOnaMztlx4yLZbbxxtEq++KLSkVWXQYNgjz0qlzwg\n/l3+8hf46KPKxVCMsWPjy8khh8Tk2BdfbDx5QPw9/uEPcOKJkWikbVICofQhvOXUvXtcThgzJj6g\n1lkHrr02Kgm3dVOmxKirSi8526MHHH109Bu0BpMmxfIEO+4Yt3Hjoi+nXTOfDrvuGqPPNIG27VIC\nIZshvOXWs2eMNBo5Moo5brBBlNOogSuSRfvtb+EnP4k1zSvtjDNihcO33qp0JEv2xRfRWtp446gs\nPX58FJzs2DH9Ma66Cm66KVov0vYogdC6WiANbbIJPPgg/OlPcMUVMf/ksccqHVX5ffBBlGo/99xK\nRxJWWAEGDqzOpW/nzYvhuOuuG63YMWOir6N795Yfq0ePmG9z/PFt+8tLW6UEQutsgTTUr19Umj31\n1FjsaK+9omXSVq5PX3gh/OIXsMoqlY5ksZNPhkcfhZdfrnQkiw0fHqP4HnwQHn44WrGlDsUdMAA+\n/zwqU0vbknsCMbM9zOxNMxtvZmc28nw3MxtuZi+Z2atmdniyfV0zG2NmLyY/p5vZSXnE2JpbIIXa\ntYs1KsaOjdnwv/1tfKD277+436QWy46PHRsfjNU2sa1rVzjrrOpYdGzRomidnXgi3HhjVCbedNNs\njr3UUlEH7vTTo3S+tB25DuM1s3bAeGAXYBIwGjjI3d8s2OdsoJu7n21mKwLjgFXcfUGD43wEfM/d\nP2zkPCUN4z388Og8PPLIog9RtWbMiAKPI0bEbdq0SCi77RadoKutVukIS/ejH8WluzPOqHQk3zRn\nTlwqGjYMttuuMjHMmBGd4p99Fv0yeX1Z+sUv4kvMH/6Qz/ElW61hGG8fYIK7v+/u84FhwL4N9nGg\na3K/K/BZYfJI9Afebix5ZKFWWiCN6do1Vt+77roYXTN6dFzuevDB6D/p3RtOOSUe15dfaU2efx6e\nfhpOOKHSkTSuY8eYlV6ppW/feQe23Tb+vkeNyvfv/NJL4R//iL8xaRvyTiCrAYUf+h8l2wpdD2xo\nZpOAl4GBjRznQOD2XCKkNvpA0urVK2o13Xln/N633hozpi+/PC539esXHwQvvNA6Lnedc05cIurU\nqdKRLNlhh8WaJCNHlve8jz0WrZ7jjovLVh065Hu+7t3j7+i447S4WltRDcUUdwfGuHs/M1sLGGlm\nm7j7TAAzWxrYBzirqYNccMEFX93v27cvffv2TR1ALbdAmtK+fZTd2HrrWBRrxgx4/PH4oDv00Ljc\n9dhjMUS4Gj36aHzDPuqoSkfStKWWijkhxx8fH+S77JLv+dzjMtLFF0cZnH798j1foZ/+FG6+OaoS\nV2ursK2qq6ujLusFi9w9txuwDfBQweOzgDMb7HM/sH3B41HAVgWP9yk8xhLO46VYZhn3GTNKOkRN\nuuYa97593RctqnQk37RokXufPu5Dh1Y6knQWLXIfNsx9zTXd99jD/eWX8znP3Lnuxxzj3ru3+9tv\n53OO5rz2mvuKK7pPmlSZ80s6yedmSZ/xeV/CGg2sbWa9zKwDcBAwvME+7xN9HJjZKsC6wDsFzx9M\njpevvvwyvrF17pzXGVqvAQNg+vRYbbHa3HdfdFAfdFClI0nHDA48MEaM/eAHMYDh8MNj/kpWpkyJ\nARL/+1/0C625ZnbHbonevaNVeNpplTm/lFFT2QVYHfglcB+RDJ4A/gDsBbRLk6GAPYiRVROAs5Jt\nxwLHJPdXBR4GXkluBxe8thMwBejazDmKzsLvvOPes2fRL695zz7r/u1vu0+dWulIFluwwH3DDd3v\nv7/SkRRv+nT3X/3Kffnl3U8/vfT396WX3NdYw/3cc90XLswmxlLMnOneq5f7I49UOhJZEjJogTT1\noXwLMAI4CdgOWBvYCNgPuA54Ctix1ACyuJWSQJ591n3LLYt+eZtw3HHuxx5b6SgWGzLEffvtq/PS\nWkt99JH7z38el3yuvNJ99uyWH+Puu+P1w4ZlH18p7rvPfd113efMqXQk31R/SfHjjysdSeXknUA2\navKF0AFYu9QAsriVkkD+9S/3Pfcs+uVtwrRp0Qp55plKRxIfRmus4f7EE5WOJFuvv+6+zz7xrf1v\nf0vXili40P3886MF/cILeUdYnL33dr/44kpH8XVz5rgfeWS0/nbeOVq0bVEWCWSJfSDu/hqAme2d\nTORr+Pw8d6/iUnHpZLWUbS1bbrmos/WLX8RqdJV0000xKqzWVmfccMPo1/nb36Ki8JZbNj3sd+ZM\nOOCA2Oe5576+2FM1ufZauOaaGC1XDSZPjlFw06ZFTIsWxbB1KU6aTvQDgQlmdrmZrZ93QOXWVofw\nttQhh8Q4/8GDKxfDzJlRnuU3v6lcDHn7/vejA/zcc2MQw267RQmaQu+9B9tvH/8ejz5aXfW/Glpj\nDfjlL2NIr1e42OLLL0fFgn794O67Ydllo37X9dfDf/5T2dharTTNFKAb0fH9DPA0cAzNdGyX80YJ\nl7BOO839ssuKfnmbMnas+worxHX7SrjkEvcDD6zMuSth3jz3wYPj8uEhh7i/+657XV08vvba1tMH\nNHduDHq4557KxXDPPdFPdMcd33zu/vvjMuBnn5U/rkoig0tYqWthmdkKwE+Bk4GxRKf6te5+XdZJ\nraVKqYV12GGw884xpFKad+65sW7EnXeW97xTp0ZNqaefjkW02pIZM2J53Ouvj0mJQ4fGcN3W5PHH\nY5LhG29Aly7lO697LHj1pz/F+u1bbtn4fqeeCu++G6VYqnll0iyVZU10M9sHOIJIGH8Fhrj7ZDPr\nBLzh7muUEkAWSkkge+4Zzeu99so4qBo1e3YsFzt4cCwdWy5nnhklw2+8sXznrDaTJ8cHYjVfsmrK\nz34Wl4vLtVb8rFmL59rcey+suuqS9507N8q+HHlkXDpsC8qVQIYAN7v7E408t4u7jyolgCyUkkC2\n2irKLmy9dcZB1bB//zuS7muvwTLL5H++iRNj1bxXX62N6sFt1eTJ8eXjkUeikGeePvww1nffaKMY\neJFmlcUJEyKJjBwJm22Wb3zVoFzVeC8Anis46TJmtgZANSSPUrWlQopZ2XPPGPVTrtErF18cM5uV\nPFq3lVeGiy6KYot5Fup8+ulYq/3gg2HIkPRL9K6zTowYO+ig1lmZuhLStECeB7Zz93nJ4w7Af929\nar6zF9sCcY8qrp9+qlImLTVxYixI9NRT0TeRl/Hj41vhuHGxTKy0bosWRXn5ww6LRJJ1f8OQIbGw\n1S23FH9Z+ogjIq6//CXb2Boza1YULF20aPF7Ybb4Vvi4qftrrRUj3lqiXJewXnL3zRpse9ndM1rP\nrHTFJpAZM+Db39a3jWJdfTU88EA0+fPoeHz2Wdh//1hX/Jhjsj++VMbLL8N++0V/Wv/+UResf/+m\n+yias3Bh9JP985+Ll+0t1syZcWn717+O4et5+eCDuMy2zDKw/PKxzX3xcOf6+w0fN3b/yCOjgnZL\nZJFA0gyRHQnsU/B4X2BUqcO/srxR5DDet9+OWc1SnPnz3TfdNJ+KuEOGuK+0UlQKkNr01lvuN9zg\nvt9+7t27u2+0kfspp7g/+GDU0krr88+jmkS/fu6ffppNbGPGxLDfCROyOV5Djz8ew7EHDarccGzy\nLGXiiz+c1yLmf3xALA71FFVSwqQgxqLewKefdt9666JeKomnn3ZfddUod5KFBQtibs5aa0V5D2kb\nFiyIUjkXX+y+447unTu777RTzP159tkllxsZP959/fXdBwyIeTNZuu66qJM3d262x73hhvhy9NBD\n2R63pbJIIC2ZB9Il+aSeWVKTJwfFXsL617/gj3+MyzBSvGOPhaWXjnkKpfj88+j4nD8/5pnUN+ul\n7Zk5c/HiZiNHRon6nXeOy1277hql6keNgp/8BC68MMrsZM0d/t//i/6FQYNKP968eXDSSfDEE1G2\nptLzmcrSB5KcaC+gN/DVeAZ3v6iUE2ep2ARy883w5JPR4SbFmzo11oD417/i2nExxo2DffaJuSWD\nBsWEOZF6EyfG8N+RI+Nnp07RAT1sGLRg8dEWmzoVNt88VngsZa7Y5Mnwox9FXbnbboNu3bKLsVhl\nGcZrZn8k6mGdCBhwANCrlJNWCxVSzMbyy8Nll8W3wGLWwn7ooagBdcYZ8PvfK3nIN622Wozcuu22\nWF/+vvuiRlieyQPib3vo0BhGPnFicccYMybmme24Y3TyV0PyyEqaeSDbufvPgGnufiGwLbFqYKun\nQorZ+elPo0TFDTekf417tDaOPDJmClf72uZSHcxiYmkpo7ZaYocdYuLsoYe2/AvSHXdEQcwrroiS\nKu3yXgO2zNL8OnOSn7PMrAcwn1hFsNXTJMLsmEXyuPDC+IbYnDlzoszE0KHwzDNRXVakWp19dvz8\n7W/T7b9wIZxzTgwtHjkSfvzj/GKrpDQJ5F9mthxwBfAi8B7w9zyDKhe1QLK1wQZw9NFRmK4pkybF\npYc5c6IPqmfPsoQnUrT27ePLzuDBzZd+nz495nc89RSMHl3bZVGaTCDJQlKj3P1zd7+H6PtY393P\nK0t0OVMLJHvnnhstiiUthjR6dJSZ2Hvv6ADt1Km88YkUq0ePGHhz6KHRud6Y8ePj77tXr/g/UOuf\nL00mEHdfBAwueDzX3afnHlWZqAWSvU6dYjjvgAHRwig0dGiMZLn+evjVr9pO2WypHXvtFaOpjjzy\nmwtkPfRQ9Jecemq0VJZeujIxllOaUiZXEotI/aOosbJlUMwwXvcosvb55+WpKNvW7LdfNN3PO2/x\n9eC7747RMxttVOnoRIo3b17UZzv88MUrLV55JVx1Vcxfai3LLZerFtYMoDOwgOhQN2IGY9UMRism\ngXzxRQwNnDEjp6DauA8/jPHzDz8cSWT2bLjrLhVElNrw1ltRFHL48GhRv/lmjCRsTf15ZZkH4u5d\n3b2du3dw927J46pJHsVS/0e+vvOdGLnSp0/MGn74YSUPqR1rrx1zlrbfPirp/uc/rSt5ZKXZKVtm\ntmNj272RBaZaE/V/5G/gwOhQ3GGHSkcikr2f/CSWMthyy7bbn5dmzu/pBfc7An2AF4B+uURUJmqB\n5G+ppZQ8pLYVW7qnVjSbQNx978LHZvYd4JrcIioTtUBEREpTzMT6j4ANsg6k3NQCEREpTZo+kOuA\n+iFO7YDNiBnprdqUKbD66pWOQkSk9UrTB/J8wf0FwO3u/t+c4imbyZNhiy0qHYWISOuVJoHcDcxx\n94UAZtbezDq5+6x8Q8uXSrmLiJQmTR/IKKBwrvYywCP5hFM+6kQXESlNmgTSsXAZ2+R+qy+Bp050\nEZHSpEkgX5rZV70FZrYlMDu/kPLnrktYIiKlStMHcjJwl5lNIupgfZtY4rbVmj49Cil27Nj8viIi\n0rg0EwlHm9n6wHrJpnHuPj/fsPKl/g8RkdI1ewnLzAYAnd39NXd/DehiZsenPYGZ7WFmb5rZeDM7\ns5Hnu5nZcDN7ycxeNbPDC55b1szuMrOxZva6mX0v7Xmbov4PEZHSpekDOdrdP69/4O7TgKPTHDxZ\n0fB6YHegN3Bw0popNAB43d03A3YGBplZfcvo98CD7r4BsCkwNs15m6MWiIhI6dIkkPZmi2tNmll7\noEPK4/cBJrj7+8llr2HAvg32caBrcr8r8Jm7LzCzbsD33f0WAHdf4O5fpDxvk9QCEREpXZoE8hBw\nh5ntYma7ALcn29JYDfiw4PFHybZC1wMbJp30LwMDk+3fBT41s1vM7EUzu8nMMlk7UC0QEZHSpUkg\nZwKPAcclt1HAGRnGsDswxt17AJsDg82sC9HBvwUw2N23AGYBZ2VxQrVARERKl2YU1iLghuTWUhOB\nwnW6Vk+2FToCuDQ519tm9i6wPtFy+dDd62tx3U0ks0ZdcMEFX93v27cvffv2XWJQU6bESnkiIm1F\nXV0ddXV1mR4zzZro6xAf8BsSC0oB4O5rNnvw6C8ZB+wCfAw8Bxzs7mML9hkMTHb3C81sFaJ446bu\nPtXMHic68ceb2flAJ3dvbCRXi9ZE798fzjgDdtst9UtERGpKFmuip5lIeAtwPnA1MUrqCFKuI+Lu\nC83sBGBE8pqb3X2smR0bT/tNwCXArWb2SvKyM9x9anL/JGComS0NvJOcu2SahS4iUro0LZAX3H1L\nM3vV3Tcu3FaWCFNoaQtk1VXh+edhtYbd+SIibUS5WiBzk/kcE5LWxESgSyknrSR3+PRTWHHFSkci\nItK6pbkUNZCovnsSsCVwKHBYnkHl6fPPoXNn+Na3Kh2JiEjrlqoWVnJ3Jhn1QVSShvCKiGQjVWd4\nLdEkQhGRbLS5BKIWiIhINtpcAlELREQkG832gZjZSkT13TUK93f3I/MLKz9qgYiIZCPNMN77gP8A\njwAL8w0nf1OmwFprVToKEZHWL00CabR8SGs1eTJss02loxARaf3S9IHcb2Y/yD2SMlEfiIhINtJO\nJLzfzOaY2YzklsnCTpWgPhARkWykmUjYtbl9WhO1QEREstFsMUUAM9sH2DF5WOfu9+caVQulLaa4\naFGUMJk1C5ZeugyBiYhUqSyKKTZ7CcvMfkdcxnojuQ00s0tLOWmlTJsGXbooeYiIZCHNKKwfAJsl\nKxNiZkOAMcDZeQaWB12+EhHJTtqZ6MsV3F82j0DKQR3oIiLZSdMCuRQYY2aPAUb0hZyVa1Q5UQtE\nRCQ7aUZh3W5mdcDWyaYz3f1/uUaVE7VARESys8RLWGa2fvJzC2BV4KPk1iPZ1uqoBSIikp2mWiCn\nAscAgxp5zoF+uUSUo8mTYd11Kx2FiEhtWGICcfdjkrt7uvucwufMrGOuUeVkyhTYYYdKRyEiUhvS\njMJ6KuW2qqc+EBGR7CyxBWJm3wZWA5Yxs82JEVgA3YBOZYgtc+oDERHJTlN9ILsDhwOrA1cVbJ8B\nnJNjTLlRC0REJDvN1sIys/3d/Z4yxVOUNLWwFi6Ejh1h9mxYKs3sFxGRGpZFLaw080DuMbO9gN5A\nx4LtF5Vy4nKbOhW6dVPyEBHJSppiin8EDgROJPpBDgB65RxX5tT/ISKSrTSjsLZz958B09z9QmBb\noNXNplD/h4hIttIkkNnJz1lm1gOYT8xMb1WmTFECERHJUpoegfvNbDngCuBFYhb6n3ONKge6hCUi\nkq00negXJ3fvMbP7gY7uPj3fsLKnS1giItlqaiLhfk08h7v/I5+Q8jFlCmywQaWjEBGpHU21QPZO\nfq4MbAc8mjzemShl0qoSyOTJsOOOze8nIiLpNFVM8QgAMxsBbOjuHyePVwVuLUt0GVIfiIhIttKM\nwvpOffJIfAL0zCme3KgPREQkW2lGYY0ys4eB25PHBwKP5BdSPtQCERHJVrO1sOCrDvXvJw+fcPd7\nU5/AbA/gGqK1c7O7X9bg+W7AbUSrpj0wyN1vTZ57D5gOLALmu3ufJZyjyVpYCxfCt74Fc+dC+/Zp\nIxcRqV1Z1MJKlUCKPrhZO2A8sAswCRgNHOTubxbsczbQzd3PNrMVgXHAKu6+wMzeAbZ092nNnKfJ\nBDJ5MvTuHa0QERHJJoE0tSb6k8nPGWb2RcFthpl9kfL4fYAJ7v6+u88HhgH7NtjHga7J/a7AZ+6+\noD6MpmJMS/0fIiLZa2oU1g7Jz65L2ieF1YAPCx5/RCSVQtcDw81sEtCF6GP5KgxgpJktBG5y9z8V\nE4T6P0REstfURMLlm3qhu0/NKIbdgTHu3s/M1iISxibuPhPY3t0/NrOVku1j3f3Jlp5ALRARkew1\nNQrrBaIF0Ng1MgfWTHH8iXx9yO/qybZCRwCXArj722b2LrA+8Hz98GF3n2Jm9xKtl0YTyAUXXPDV\n/b59+9K3b9+vHqsFIiJtXV1dHXV1dZkeM+9O9PZEp/guwMfAc8DB7j62YJ/BwGR3v9DMVgGeBzYF\n5gDt3H2mmXUGRgAXuvuIRs7TZCf6eedBu3ZQkGNERNq0sqxImJyoO7AOX1+R8InmXufuC83sBOLD\nv34Y71gzOzae9puAS4BbzeyV5GVnuPtUM/sucK+ZeRLn0MaSRxpTpsBGGxXzShERWZJmE4iZ/RwY\nSFx+egnYBnga6JfmBO7+ELBeg203Ftz/mOgHafi6d4HN0pyjObqEJSKSvTRDZAcCWwPvu/vOwObA\n57lGlTF1oouIZC9NApnj7nMAzOxbySTA9Zp5TVVRC0REJHtp+kA+SlYk/CcxlHYa8H6+YWVLLRAR\nkey1aBSWme0ELAs85O7zcouqhZoahbVgASyzDMyZozpYIiL1yjIKy8yuBYa5+1Pu/ngpJ6uETz+F\n7t2VPEQNJXVmAAAQh0lEQVREspamD+QF4Fwze9vMrjSzrfIOKkvq/xARyUezCcTdh7j7D4iRWOOA\ny8xsQu6RZUT9HyIi+WhJpdu1iRIjvYA3m9m3aqgFIiKSj2YTiJldnrQ4LgJeA7Zy971zjywjaoGI\niOQjzTDet4Ft3f3TvIPJg1ogIiL5SNMHcmN98jCzC3KPKGNqgYiI5KOlq/3tk0sUOVILREQkHy1N\nICVNOqmEKVPUAhERyUNLE8iWuUSRI13CEhHJR9pRWN3MbGmiFtYUMzu0DLFlQpewRETykaYFspu7\nfwH8H/AeMR/k9DyDysr8+TBjRpQyERGRbKVJIPVDffcC7nL36TnGk6lPP4UVVojlbEVEJFtp5oHc\nb2ZvArOB48xsJWK98qqn/g8RkfykmQdyFrAdMQN9PvAlsG/egWVB/R8iIvlJ04l+ADDf3Rea2bnA\nbUCP3CPLgFogIiL5SdM78Gt3n2FmOwD9gZuBG/INKxtqgYiI5CdNAlmY/NwLuMndHwA65BdSdtQC\nERHJT5oEMtHMbgQOBB40s2+lfF3FqQUiIpKfNIngx8DDwO7u/jmwPK1kHohaICIi+UkzCmsWUdJ9\ndzM7AVjZ3UfkHlkG1AIREclPmlFYA4GhwMrJ7TYzOzHvwLKgFoiISH7M3ZvewewVYkGpL5PHnYGn\n3X2TMsSXipl5Y7/HcsvBu++qlImISENmhruXVGE9TR+IsXgkFsn9qi/rPm8ezJoVSURERLKXppTJ\nLcCzZnZv8viHxFyQqjZlStTBsqpPdSIirVOzCcTdrzKzOmCHZNMR7j4m16gyoA50EZF8NZlAzKw9\n8Lq7rw+8WJ6QsqEOdBGRfDXZB+LuC4FxZtazTPFkRi0QEZF8pekD6Q68bmbPEZV4AXD3fXKLKgNq\ngYiI5CtNAvl17lHkQC0QEZF8LTGBmNnawCru/niD7TsAH+cdWKkmT4Y+fSodhYhI7WqqD+Qa4ItG\ntk9PnqtqaoGIiOSrqQSyiru/2nBjsm2NtCcwsz3M7E0zG29mZzbyfDczG25mL5nZq2Z2eIPn25nZ\ni2Y2PO05QX0gIiJ5ayqBNDWHe5k0BzezdsD1wO5Ab+BgM1u/wW4DiKHCmwE7A4PMrPDS2kDgjTTn\nK6QWiIhIvppKIM+b2dENN5rZz4EXUh6/DzDB3d9P1lMfxjfXU3ega3K/K/CZuy9IzrU68APgzynP\n9xW1QERE8tXUKKyTgXvN7BAWJ4ytiNUI/1/K468GfFjw+CMiqRS6HhhuZpOALsTCVfWuJtYeWTbl\n+QCYOxfmzIFlW/QqERFpiSUmEHf/BNjOzHYGNko2P+Duj2Ycw+7AGHfvZ2ZrASPNbBNgJ+ATd3/J\nzPrSTAHHCy644Kv7vXv3ZaWV+qoOlohIoq6ujrq6ukyP2Ww595IObrYNcIG775E8Pgtwd7+sYJ/7\ngUvd/b/J41HAmcB+wKHAAqLPpSvwD3f/WSPn+Vo59xdfhKOOgjFVX7FLRKQyylXOvRSjgbXNrJeZ\ndQAOAhqOpnof6A9gZqsA6wLvuPs57t7T3ddMXvdoY8mjMVOmqP9DRCRvaWaiF83dFybL4I4gktXN\n7j7WzI6Np/0m4BLg1mThKoAz3H1qKedVB7qISP5yvYRVLg0vYV11FXz4IVx9dQWDEhGpYq3hElZF\nqAUiIpK/mkwgmkQoIpK/mkwgaoGIiOSvJhOIWiAiIvmryQSiFoiISP5qMoGoBSIikr+aSyCzZ8O8\nedC1a/P7iohI8WougdS3PlQHS0QkXzWXQNT/ISJSHjWXQNT/ISJSHjWZQNQCERHJX80lkMmT1QIR\nESmHmksgaoGIiJRHzSUQdaKLiJRHzSUQdaKLiJRHzSUQtUBERMqj5hKIWiAiIuVRcwlELRARkfKo\nqQTy5ZewaBF06VLpSEREal9NJZD6IbyqgyUikr+aSyDq/xARKY+aSiDq/xARKZ+aSiBqgYiIlE9N\nJRC1QEREyqemEohaICIi5bNUpQPI0m67wfLLVzoKEZG2wdy90jGUzMy8Fn4PEZFyMTPcvaRJDzV1\nCUtERMpHCURERIqiBCIiIkVRAhERkaIogYiISFGUQEREpChKICIiUhQlEBERKUruCcTM9jCzN81s\nvJmd2cjz3cxsuJm9ZGavmtnhyfZvmdmzZjYm2X5+3rGKiEh6uSYQM2sHXA/sDvQGDjaz9RvsNgB4\n3d03A3YGBpnZUu4+F9jZ3TcHNgP2NLM+ecZbC+rq6iodQlXQ+7CY3ovF9F5kK+8WSB9ggru/7+7z\ngWHAvg32caBrcr8r8Jm7LwBw91nJ9m8RdbtUr6QZ+g8S9D4spvdiMb0X2co7gawGfFjw+KNkW6Hr\ngQ3NbBLwMjCw/gkza2dmY4D/ASPdfXTO8YqISErV0Im+OzDG3XsAmwODzawLgLsvSi5hrQ58z8w2\nrGCcIiJSINdqvGa2DXCBu++RPD4LcHe/rGCf+4FL3f2/yeNRwJnu/nyDY/0a+NLdr2rkPLq0JSLS\nQqVW4817PZDRwNpm1gv4GDgIOLjBPu8D/YH/mtkqwLrAO2a2IjDf3aeb2TLArsDvGjtJqW+CiIi0\nXK4JxN0XmtkJwAjictnN7j7WzI6Np/0m4BLgVjN7JXnZGe4+1cw2BoYkI7naAXe4+4N5xisiIunV\nxIJSIiJSftXQiV605iYp1jIzW93MHjWz15OJlicl27ub2QgzG2dmD5vZspWOtVySUXsvmtnw5HGb\nfC/MbFkzu8vMxiZ/H99rw+/FKWb2mpm9YmZDzaxDW3kvzOxmM/uk4OpOk/8nzOxsM5uQ/N3sluYc\nrTaBpJykWMsWAKe6e29gW2BA8vufBTzi7usBjwJnVzDGchsIvFHwuK2+F78HHnT3DYBNgTdpg++F\nmfUATgS2cPdNiEv2B9N23otbiM/HQo3+7skI1x8DGwB7An8ws2b7llttAiHdJMWa5e7/c/eXkvsz\ngbHEcOd9gSHJbkOAH1YmwvIys9WBHwB/Ltjc5t4LM+sGfN/dbwFw9wXuPp02+F4k2gOdzWwpYBlg\nIm3kvXD3J4FpDTYv6XffBxiW/L28B0wgPmOb1JoTSJpJim2Cma1BlHt5BljF3T+BSDLAypWLrKyu\nBk7n69UK2uJ78V3gUzO7Jbmcd5OZdaINvhfuPgkYBHxAJI7p7v4IbfC9KLDyEn73hp+nE0nxedqa\nE4gAyaTLu4GBSUuk4aiImh8lYWZ7AZ8kLbKmmt01/14Ql2m2AAa7+xbAl8Rli7b4d7Ec8Y27F9CD\naIkcQht8L5pQ0u/emhPIRKBnwePVk21tRtIsvxv4m7vfl2z+JJlPg5l9G5hcqfjKaHtgHzN7B7gd\n6GdmfwP+1wbfi4+ADwsm4t5DJJS2+HfRH3jH3ae6+0LgXmA72uZ7UW9Jv/tE4DsF+6X6PG3NCeSr\nSYpm1oGYpDi8wjGV21+AN9z99wXbhgOHJ/cPA+5r+KJa4+7nuHtPd1+T+Dt41N1/CvyLtvdefAJ8\naGbrJpt2AV6nDf5dEJeutjGzjkmH8C7EIIu29F4YX2+VL+l3Hw4clIxS+y6wNvBcswdvzfNAzGwP\nYsRJ/STFRmeq1yIz2x54AniVaIY6cA7xj34n8W3ifeDH7v55peIsNzPbCTjN3fcxs+Vpg++FmW1K\nDCZYGngHOILoTG6L78X5xJeK+cAY4OdE1e+afy/M7O9AX2AF4BPgfOCfwF008rub2dnAUcR7NdDd\nRzR7jtacQEREpHJa8yUsERGpICUQEREpihKIiIgURQlERESKogQiIiJFUQIREZGiKIFIJsxskZld\nUfD4NDM7L6Nj32Jm+2VxrGbO8yMzeyNZVrnhc1ckZfMva+y1zRx3UzPbM5so82FmM4p83b7FVMEu\n9nxSXZRAJCtzgf2SyXtVw8zat2D3o4Cfu/sujTx3NLCJuxez7sxmRKXgFklTTjtDxU4I+yGxnEK5\nzidVRAlEsrIAuAk4teETDVsQ9d8+zWwnM6szs3+a2VtmdqmZ/cTMnjWzl5OSCvV2NbPRFguI7ZW8\nvp2ZXZ7s/5KZHV1w3CfM7D6ijEfDeA5OFhh6xcwuTbb9GtgBuLlhKyM5ThfgBTM7wMxWNLO7k/M+\na2bbJvttbWZPmdkLZvakma1jZksDFwE/TqrjHmBm55vZqQXHf9XMeiZled40syFm9iqwupntmhzz\neTO7I6msi5n9zmKhpJfM7PJGfscdzWxMcs4XzKxzsv2XZvZc8rrzG/uHXNI+Zvaz5N9lTBLjtkQZ\n8MuT83zXzNY0s38n/1aP15dUMbM1kt/jZTO7uLHzSivk7rrpVvIN+IL4kH2XKBVxGnBe8twtwH6F\n+yY/dwKmEiWlOxCFAM9PnjsJuKrg9Q8m99cmyk53IFoF5yTbOxD10Xolx50B9GwkzlWJEg7LE1+g\nRgH7JM89Bmy+pN+v4P5QYLvk/neIemQkv3+75P4uwN3J/cOAawtefz6xGFj941eIwqC9iES8dbJ9\nBeBxYJnk8RnAuUnsbxa8vlsj8Q4Htk3udyJKmewK3JhsM6JW2A4N/k0a3QfYkFiYqnvy3HJL+Ld9\nBFgrud8HGJXcvw84JLl/fOH7qVvrvS2FSEbcfaaZDSFWBpyd8mWj3X0ygJm9DdTX33mVqONT787k\nHG8l+60P7AZsbGYHJPt0A9Yhavk85+4fNHK+rYHH3H1qcs6hwI4sLsS5pMtGhdv7AxsUXGLqkrQM\nlgP+ambrEJdo0v7/Kjz2++4+Orm/DfHB/d/kXEsDTwHTgdlm9mfgAeD+Ro75X+Dq5Pf7h7tPtFim\ndFczezE5Z2fi/Xqy4HVL2qczcJe7TwPwRmpHJa2c7YC7Ct6bpZOf2wP1rdC/AW2mbl0tUwKRrP0e\neJH4ZlpvAcnl0uSDpUPBc3ML7i8qeLyIr/99Fl4zt+SxASe6+8jCACwKKn7ZRIzF9C00PP/3PFbC\nLDzvYKIS8H5m1oto0TTmq/cj0bHgfmHcBoxw90MaHsDM+hCtnAOAE5L7i4N1v8zM7gf2Ap60KDxq\nwKXu/qcl/5qN72NmJzTxmnrtgGke65A0VF/ws/4cUgPUByJZMYDkG+qdRId0vfeArZL7+7L4W2lL\nHGBhLWLVvXHAw8DxFuuikPQ5dGrmOM8BO5rZ8hYd7AcDdSnOX/ihN4JoZZGcd9PkbjcWr6FwRMH+\nM5Ln6r1HrNGBmW2R/D6NnecZYPvkd8bMOiW/Y2fiEtJDRJ/TJt8I1mxNd3/d3S8HngfWI96vIwv6\nQ3qY2YoNztvYPisR62cfYMkgCTPr3vB3c/cZwLtm9qOCOOpj+y/xXgN8IyFK66QEIlkp/IY+iLh+\nX7/tT8BOZjaGuCyzpNZBUyNzPiA+/B8AjnX3eUTJ8jeAF5NO5z8S1/qXHGQs43kWkTTGEJfQ6i8B\nNXX+wucGAlslHcKvAccm268AfmdmL/D1/1uPARvWd6ITizytkMR8PJEMv3Eed/+UWLvhdjN7mbh8\ntR7Rx3R/su0J4JRG4j056Zx/CZgH/Dtpqf0deNrMXiHKenctPO8S9uni7m8AvwEeT/4dByWvGwac\nnnTUf5dIDkclHfCvEZ3sACcDA5KYV230HZZWR+XcRUSkKGqBiIhIUZRARESkKEogIiJSFCUQEREp\nihKIiIgURQlERESKogQiIiJFUQIREZGi/H+7syuszHWYDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195221ee5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "percentile_step = 5 # try 1, 2, 5, 10, etc.\n",
    "\n",
    "percentiles = range(1, 100, percentile_step)\n",
    "results = []\n",
    "for i in range(1,100,percentile_step):\n",
    "    fs = feature_selection.SelectPercentile(\n",
    "        feature_selection.chi2, percentile=i)\n",
    "    X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "    scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "    print (i, scores.mean())\n",
    "    results = np.append(results, scores.mean())\n",
    "\n",
    "optimal_percentil = np.where(results == results.max())[0]\n",
    "print (\"Optimal number of features:{0}\".format(percentiles[optimal_percentil[0]]), \"\\n\")\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "import pylab as pl\n",
    "pl.figure()\n",
    "pl.xlabel(\"Number of features selected\")\n",
    "pl.ylabel(\"Cross-validation accuracy)\")\n",
    "pl.plot(percentiles, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that accuracy quickly improves when we start adding features, remaining\n",
    "stable after the percentile of features turns about 10. In fact, the best accuracy is\n",
    "achieved when using 64 of the original 581 features (at the 11 percent percentile).\n",
    "Let's see if this actually improved performance on the testing set.\n",
    "\n",
    "Note: results are diffrent from statement above. Probably Python 2 and Python 3 calculate _little difffrent_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=percentiles[optimal_percentil[0]])\n",
    "X_train_fs = fs.fit_transform(X_train, y_train)\n",
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y_test, y_pred_fs)), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance improved slightly, again. Compared with our initial performance,\n",
    "we have finally improved by almost one accuracy point using only 11 percent of\n",
    "the features.\n",
    "\n",
    "The reader may have noted that while creating our classifier, we used the default\n",
    "parameters, except for the splitting criterion, where we have used entropy. Can we\n",
    "improve our model using different parameters? This task is called model selection,\n",
    "and we will address it in detail in the next section using a different learning example.\n",
    "\n",
    "For now, let's just test if the alternative method (gini) would result in better\n",
    "performance for our example. To do this, we will again use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy criterion accuracy on cv: 0.877\n",
      "Gini criterion accuracy on cv: 0.880\n"
     ]
    }
   ],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print (\"Entropy criterion accuracy on cv: {0:.3f}\".format(scores.mean()))\n",
    "dt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=5)\n",
    "print (\"Gini criterion accuracy on cv: {0:.3f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini criterion performs better on our training set. How about its performance on\n",
    "the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train_fs, y_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "y_pred_fs = dt.predict(X_test_fs)\n",
    "print (\"Accuracy: {0:.3f}\".format(metrics.accuracy_score(y_test,y_pred_fs)),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that performance improvement on the training set did not hold for the\n",
    "evaluation set. This is always possible. In fact, performance could have decreased\n",
    "(recall overfitting). Our model is still the best. If we changed our model to use the one\n",
    "with the best performance in the testing set, we can never measure its performance,\n",
    "since the testing dataset could not be considered \"unseen data\" anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we worked on ways to preprocess the data and select the\n",
    "most promising features. As we stated, selecting a good set of features is a crucial\n",
    "step to obtain good results. Now we will focus on another important step: selecting\n",
    "the algorithm parameters, known as __hyperparameters__ to distinguish them from the\n",
    "parameters that are adjusted within the machine learning algorithm. Many machine\n",
    "learning algorithms include hyperparameters (from now on we will simply call them\n",
    "parameters) that guide certain aspects of the underlying method and have great\n",
    "impact on the results. In this section we will review some methods to help us obtain\n",
    "the best parameter configuration, a process known as model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look back at the text-classification problem we addressed in Chapter\n",
    "2, Supervised Learning. In that example, we compounded a TF-IDF vectorizer\n",
    "alongside a multinomial Naïve Bayes (NB) algorithm to classify a set of newsgroup\n",
    "messages into a discrete number of categories. The MultinomialNB algorithm has\n",
    "one important parameter, named alpha, that adjusts the smoothing. We initially\n",
    "used the class with its default parameter values (alpha = 1.0) and obtained an\n",
    "accuracy of 0.89. But when we set alpha to 0.01, we obtained a noticeable accuracy\n",
    "improvement to 0.92. Clearly, the configuration of the alpha parameter has great\n",
    "impact on the performance of the algorithm. __How can we be sure 0.01 is the best\n",
    "value?__ Perhaps if we try other possible values, we could still obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start again with our text-classification problem, but for now we will only use a\n",
    "reduced number of instances. We will work only with 3,000 instances. We start by\n",
    "importing our pylab environment and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "n_samples = 3000\n",
    "X_train = news.data[:n_samples]\n",
    "y_train = news.target[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the set of stop words and create a pipeline that compounds the TF-IDF\n",
    "vectorizer and the Naïve Bayes algorithms (recall that we had a stopwords_en.txt\n",
    "file with a list of stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open(r'data/stopwords_en.txt', 'r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result\n",
    " \n",
    "stop_words = get_stop_words()\n",
    "clf = Pipeline([\n",
    "        ('vect', TfidfVectorizer(\n",
    "            stop_words=stop_words,\n",
    "            token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "        )),\n",
    "        ('nb', MultinomialNB(alpha=0.01)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate our algorithm with a three-fold cross-validation, we obtain a mean\n",
    "score of around 0.814."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.812  0.808  0.822]\n",
      "Mean score: 0.814 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # create a k-fold croos validation iterator of k=5 folds\n",
    "    cv = KFold(len(y), K, shuffle=True, random_state=0)\n",
    "    # by default the score used is the one returned by score method of the estimator (accuracy)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print (scores)\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))\n",
    "    \n",
    "evaluate_cross_validation(clf, X_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we should train the algorithm with a list of different parameter values\n",
    "and keep the parameter value that achieves the best results. Let's implement a helper\n",
    "function to do that. This function will train the algorithm with a list of values, each\n",
    "time obtaining an accuracy score calculated by performing k-fold cross-validation\n",
    "on the training instances. After that, it will plot the training and testing scores as a\n",
    "function of the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_params(X, y, clf, param_values, param_name, K):\n",
    "    # initialize training and testing scores with zeros\n",
    "    train_scores = np.zeros(len(param_values))\n",
    "    test_scores = np.zeros(len(param_values))\n",
    "\n",
    "    # iterate over the different parameter values\n",
    "    for i, param_value in enumerate(param_values):\n",
    "        print (param_name, ' = ', param_value)\n",
    "        # set classifier parameters\n",
    "        clf.set_params(**{param_name:param_value})\n",
    "        # initialize the K scores obtained for each fold\n",
    "        k_train_scores = np.zeros(K)\n",
    "        k_test_scores = np.zeros(K)\n",
    "        # create KFold cross validation\n",
    "        cv = KFold(n_samples, K, shuffle=True, random_state=0)\n",
    "        # iterate over the K folds\n",
    "        for j, (train, test) in enumerate(cv):\n",
    "            clf.fit([X[k] for k in train], y[train])\n",
    "            k_train_scores[j] = clf.score([X[k] for k in train], y[train])\n",
    "            k_test_scores[j] = clf.score([X[k] for k in test], y[test])\n",
    "        train_scores[i] = np.mean(k_train_scores)\n",
    "        test_scores[i] = np.mean(k_test_scores)\n",
    "\n",
    "    # plot the training and testing scores in a log scale\n",
    "    plt.semilogx(param_values, train_scores, alpha=0.4, lw=2, c='b')\n",
    "    plt.semilogx(param_values, test_scores, alpha=0.4, lw=2, c='g')\n",
    "    plt.xlabel(\"Alpha values\")\n",
    "    plt.ylabel(\"Mean cross-validation accuracy\")\n",
    "    # return the training and testing scores on each parameter value\n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function accepts six arguments: the feature array, the target array, the classifier\n",
    "object to be used, the list of parameter values, the name of the parameter to adjust,\n",
    "and the number of K folds to be used in the crossvalidation evaluation.\n",
    "Let's call this function; we will use numpy's logspace function to generate a list of\n",
    "alpha values spaced evenly on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-07   1.00000000e-06   1.00000000e-05   1.00000000e-04\n",
      "   1.00000000e-03   1.00000000e-02   1.00000000e-01   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-7, 0, 8)\n",
    "print (alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the values of the alpha parameter of the NB classifier within the pipeline,\n",
    "which corresponds to the parameter name nb__alpha. We will use three folds for the\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb__alpha  =  1e-07\n",
      "nb__alpha  =  1e-06\n",
      "nb__alpha  =  1e-05\n",
      "nb__alpha  =  0.0001\n",
      "nb__alpha  =  0.001\n",
      "nb__alpha  =  0.01\n",
      "nb__alpha  =  0.1\n",
      "nb__alpha  =  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAETCAYAAAAYm1C6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19522192828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores, test_scores = calc_params(X_train, y_train, clf, alphas, 'nb__alpha', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure, the line at the top corresponds to the training accuracy and\n",
    "the one at the bottom to the testing accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the training accuracy is always greater than the testing accuracy. We\n",
    "can see in the graph that the best testing accuracy is obtained with an alpha value in\n",
    "the range of 10-2 and 10-1. Below this range, the classifier shows signs of overfitting\n",
    "(the training accuracy is high but the testing accuracy is lower than it could be).\n",
    "Above this range, the classifier shows signs of underfitting (accuracy on the training\n",
    "set is lower than it could be).\n",
    "\n",
    "It is worth mentioning that at this point a second pass could be performed in the\n",
    "range of 10-2 and 10-1with a finer grid to find an ever better alpha value.\n",
    "\n",
    "Let's print the scores vector to look at the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training scores:  [ 1.          1.          1.          1.          1.          1.\n",
      "  0.99683333  0.97416667]\n",
      "testing scores:  [ 0.77133333  0.77666667  0.78233333  0.79466667  0.80366667  0.814\n",
      "  0.80733333  0.74533333]\n"
     ]
    }
   ],
   "source": [
    "print ('training scores: ', train_scores)\n",
    "print ('testing scores: ', test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results are obtained with an alpha value of 0.1 (accuracy of 0.814)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a very useful function to graph and obtain the best parameter value for a\n",
    "classifier. Let's use it to adjust another classifier that uses a Support Vector Machines\n",
    "(SVM) instead of MultinomialNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('svc', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a pipeline as before, but now we use the SVC classifier with its default\n",
    "values. Now we will use our calc_params function to adjust the gamma parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.01   0.1    1.    10.  ]\n"
     ]
    }
   ],
   "source": [
    "gammas = np.logspace(-2, 1, 4)\n",
    "print (gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc__gamma  =  0.01\n",
      "svc__gamma  =  0.1\n",
      "svc__gamma  =  1.0\n",
      "svc__gamma  =  10.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAETCAYAAADDIPqYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nliSMMcb0yZKEMcaYPv3/g+ybE13An/YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19524b37940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores, test_scores = calc_params(X_train, y_train, clf, gammas,'svc__gamma', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gamma values lesser than one we have underfitting and for gamma values\n",
    "greater than one we have overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best result is for a gamma value of 1, where we obtain a training accuracy of\n",
    "0.999 and a testing accuracy of 0.760.\n",
    "\n",
    "If you take a closer look at the SVC class constructor parameters, we have other\n",
    "parameters, apart from gamma, that may also affect classifier performance. If we\n",
    "only adjust the gamma value, we implicitly state that the optimal C value is 1.0\n",
    "(the default value that we did not explicitly set). Perhaps we could obtain better\n",
    "results with a new combination of C and gamma values. This opens a new degree of\n",
    "complexity; we should try all the parameter combinations and keep the better one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mitigate this problem, we have a very useful class named __GridSearchCV__ within\n",
    "the __sklearn.grid_search__ module. What we have been doing with our calc_\n",
    "params function is a kind of grid search in one dimension. With GridSearchCV, we\n",
    "can specify a grid of any number of parameters and parameter values to traverse. It\n",
    "will train the classifier for each combination and obtain a cross-validation accuracy to\n",
    "evaluate each one.\n",
    "\n",
    "Let's use it to adjust the C and the gamma parameters at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "     'svc__gamma': np.logspace(-2, 1, 4),\n",
    "    'svc__C': np.logspace(-1, 1, 3),\n",
    "}\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "gs = GridSearchCV(clf, parameters, verbose=2, refit=False, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute our grid search and print the best parameter values and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] svc__gamma=0.01, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  12.9s\n",
      "[CV] svc__gamma=0.01, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  14.4s\n",
      "[CV] svc__gamma=0.01, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=0.1 -  14.2s\n",
      "[CV] svc__gamma=0.1, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  14.4s\n",
      "[CV] svc__gamma=0.1, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  13.7s\n",
      "[CV] svc__gamma=0.1, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=0.1 -  13.7s\n",
      "[CV] svc__gamma=1.0, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  14.8s\n",
      "[CV] svc__gamma=1.0, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  13.3s\n",
      "[CV] svc__gamma=1.0, svc__C=0.1 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=0.1 -  14.1s\n",
      "[CV] svc__gamma=10.0, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  13.1s\n",
      "[CV] svc__gamma=10.0, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  13.9s\n",
      "[CV] svc__gamma=10.0, svc__C=0.1 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=0.1 -  15.1s\n",
      "[CV] svc__gamma=0.01, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  13.4s\n",
      "[CV] svc__gamma=0.01, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  12.8s\n",
      "[CV] svc__gamma=0.01, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.01, svc__C=1.0 -  13.1s\n",
      "[CV] svc__gamma=0.1, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  13.7s\n",
      "[CV] svc__gamma=0.1, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  14.3s\n",
      "[CV] svc__gamma=0.1, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=0.1, svc__C=1.0 -  14.9s\n",
      "[CV] svc__gamma=1.0, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  15.2s\n",
      "[CV] svc__gamma=1.0, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  14.1s\n",
      "[CV] svc__gamma=1.0, svc__C=1.0 ......................................\n",
      "[CV] ............................. svc__gamma=1.0, svc__C=1.0 -  14.1s\n",
      "[CV] svc__gamma=10.0, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  15.4s\n",
      "[CV] svc__gamma=10.0, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  14.4s\n",
      "[CV] svc__gamma=10.0, svc__C=1.0 .....................................\n",
      "[CV] ............................ svc__gamma=10.0, svc__C=1.0 -  14.6s\n",
      "[CV] svc__gamma=0.01, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  12.9s\n",
      "[CV] svc__gamma=0.01, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  13.4s\n",
      "[CV] svc__gamma=0.01, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=0.01, svc__C=10.0 -  12.1s\n",
      "[CV] svc__gamma=0.1, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  13.6s\n",
      "[CV] svc__gamma=0.1, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  13.4s\n",
      "[CV] svc__gamma=0.1, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=0.1, svc__C=10.0 -  11.4s\n",
      "[CV] svc__gamma=1.0, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  12.3s\n",
      "[CV] svc__gamma=1.0, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  14.2s\n",
      "[CV] svc__gamma=1.0, svc__C=10.0 .....................................\n",
      "[CV] ............................ svc__gamma=1.0, svc__C=10.0 -  14.0s\n",
      "[CV] svc__gamma=10.0, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  12.3s\n",
      "[CV] svc__gamma=10.0, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  15.5s\n",
      "[CV] svc__gamma=10.0, svc__C=10.0 ....................................\n",
      "[CV] ........................... svc__gamma=10.0, svc__C=10.0 -  14.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 8min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'svc__C': 10.0, 'svc__gamma': 0.10000000000000001}, 0.8266666666666667)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time _ = gs.fit(X_train, y_train)\n",
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the grid search, we obtained a better combination of C and gamma parameters,\n",
    "for values 10.0 and 0.10 respectively, with a three-fold cross-validation accuracy of\n",
    "0.827, which is much better than the best value we obtained (0.76) in the previous\n",
    "experiment by only adjusting gamma and keeping the C value at 1.0.\n",
    "At this point, we could continue performing experiments by trying not only to adjust\n",
    "other parameters of the SVC but also adjusting the parameters on TfidfVectorizer,\n",
    "which is also part of the estimator. Note that this additionally increases the\n",
    "complexity. As you might have noticed, the previous grid search experiment took\n",
    "about five minutes to finish. If we add new parameters to adjust, the time will\n",
    "increase exponentially. As a result, these kinds of methods are very resource/time\n",
    "intensive; this is also the reason why we used only a subset of the total instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sdfsdfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdfsdfs`sadsddsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
