{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Python version: 3.5.2\n",
      "IPython version: 4.0.1\n",
      "numpy version: 1.13.1\n",
      "scikit-learn version: 0.18.2\n",
      "matplotlib version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import platform\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare several regression methods by using the same\n",
    "dataset. We will try to predict the price of a house as a function of its attributes.\n",
    "As the dataset, we will use the Boston house-prices dataset, which includes 506\n",
    "instances, representing houses in the suburbs of Boston by 14 features, one of them\n",
    "(the median value of owner-occupied homes) being the target class (for a detailed\n",
    "reference, see http://archive.ics.uci.edu/ml/datasets/Housing). Each\n",
    "attribute in this dataset is real-valued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "50.0 5.0 22.5328063241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print (boston.data.shape)\n",
    "print (boston.feature_names)\n",
    "print (np.max(boston.target), np.min(boston.target), np.mean(boston.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start slicing our learning set into training and testing datasets, and\n",
    "normalizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "scalery = StandardScaler().fit(y_train)\n",
    "X_train = scalerX.transform(X_train)\n",
    "y_train = scalery.transform(y_train)\n",
    "X_test = scalerX.transform(X_test)\n",
    "y_test = scalery.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at our best classifier, let's define how we will compare our results.\n",
    "Since we want to preserve our testing set for evaluating the performance of the final\n",
    "classifier, we should find a way to select the best model while avoiding overfitting.\n",
    "We already know the answer: cross-validation. Regression poses an additional\n",
    "problem: how should we evaluate our results? Accuracy is not a good idea, since\n",
    "we are predicting real values, it is almost impossible for us to predict exactly the\n",
    "final value. There are several measures that can be used (you can look at the list of\n",
    "functions under sklearn.metrics module). The most common is the R2 score, or\n",
    "coefficient of determination that measures the proportion of the outcomes variation\n",
    "explained by the model, and is the default score function for regression methods in\n",
    "scikit-learn. This score reaches its maximum value of 1 when the model perfectly\n",
    "predicts all the test target values. Using this measure, we will build a function that\n",
    "trains a model and evaluates its performance using five-fold cross-validation and the\n",
    "coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import *\n",
    "\n",
    "def train_and_evaluate(clf, X_train, y_train):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print (\"Coefficient of determination on training set:\",clf.score(X_train, y_train))\n",
    "    # create a k-fold cross validation iterator of k=5 folds\n",
    "    cv = KFold(X_train.shape[0], 5, shuffle=True, random_state=33)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv)\n",
    "    print (\"Average coefficient of determination using 5-fold crossvalidation:\",np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try – a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question that linear models try to answer is which hyperplane in the\n",
    "14-dimensional space created by our learning features (including the target value)\n",
    "is located closer to them. After this hyperplane is found, prediction reduces to\n",
    "calculate the projection on the hyperplane of the new point, and returning the target\n",
    "value coordinate. Think of our first example in Chapter 1, Machine Learning – A Gentle\n",
    "Introduction, where we wanted to find a line separating our training instances.\n",
    "We could have used that line to predict the second learning attribute as a function\n",
    "of the first one, that is, linear regression.\n",
    "\n",
    "But, what do we mean by closer? The usual measure is least squares: calculate the\n",
    "distance of each instance to the hyperplane, square it (to avoid sign problems), and\n",
    "sum them. The hyperplane whose sum is smaller is the least squares estimator (the\n",
    "hyperplane in the case if two dimensions are just a line).\n",
    "\n",
    "Since we don't know how our data fits (it is difficult to print a 14-dimension\n",
    "scatter plot!), we will start with a linear model called SGDRegressor, which tries to\n",
    "minimize squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.743617732983\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.710809853468\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42)\n",
    "train_and_evaluate(clf_sgd,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the hyperplane coefficients our method has calculated, which is\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08527595  0.06706144 -0.05032898  0.10874804 -0.07755151  0.38961893\n",
      " -0.02485839 -0.20990016  0.08491659 -0.05495108 -0.19854006  0.06126093\n",
      " -0.37817963]\n"
     ]
    }
   ],
   "source": [
    "print (clf_sgd.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noted the penalty=None parameter when we called the method.\n",
    "The penalization parameter for linear regression methods is introduced to avoid\n",
    "overfitting. It does this by penalizing those hyperplanes having some of their\n",
    "coefficients too large, seeking hyperplanes where each feature contributes more or less\n",
    "the same to the predicted value. This parameter is generally the L2 norm (the squared\n",
    "sums of the coefficients) or the L1 norm (that is the sum of the absolute value of the\n",
    "coefficients). Let's see how our model works if we introduce an L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.743616743208\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.71081206667\n"
     ]
    }
   ],
   "source": [
    "clf_sgd1 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', random_state=42)\n",
    "train_and_evaluate(clf_sgd1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we did not obtain an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try – Support Vector Machines for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.71886923342\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.707838419194\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf_svr = svm.SVR(kernel='linear')\n",
    "train_and_evaluate(clf_svr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we had no improvement. However, one of the main advantages of SVM is that\n",
    "(using what we called the kernel trick) we can use a nonlinear function, for example,\n",
    "a polynomial function to approximate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.904109273301\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.779288545488\n"
     ]
    }
   ],
   "source": [
    "clf_svr_poly = svm.SVR(kernel='poly')\n",
    "train_and_evaluate(clf_svr_poly, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our results are six points better in terms of coefficient of determination. We can\n",
    "actually improve this by using a Radial Basis Function (RBF) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.900132065979\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.833662221567\n"
     ]
    }
   ],
   "source": [
    "clf_svr_rbf = svm.SVR(kernel='rbf')\n",
    "train_and_evaluate(clf_svr_rbf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF kernels have been used in several problems and have shown to be very effective.\n",
    "Actually, RBF is the default kernel used by SVM methods in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third try – Random Forests revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try a very different approach to regression using Random Forests. We have\n",
    "previously used Random Forests for classification. When used for regression, the tree\n",
    "growing procedure is exactly the same, but at prediction time, when we arrive at a\n",
    "leaf, instead of reporting the majority class, we return a representative real value, for\n",
    "example, the average of the target values.\n",
    "\n",
    "Actually, we will use Extra Trees, implemented in the ExtraTreesRegressor\n",
    "class within the sklearn.ensemble module. This method adds an extra level of\n",
    "randomization. It not only selects for each tree a different, random subset of features,\n",
    "but also randomly selects the threshold for each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 1.0\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.861758978344\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "clf_et=ensemble.ExtraTreesRegressor(n_estimators=10, random_state=42)\n",
    "train_and_evaluate(clf_et, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that we have not only completely eliminated underfitting\n",
    "(achieving perfect prediction on training values), but also improved the performance\n",
    "by three points while using cross-validation. An interesting feature of Extra Trees\n",
    "is that they allow computing the importance of each feature for the regression task.\n",
    "Let's compute this importance as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0050438532027558842, 'ZN'), (0.015142513715149682, 'B'), (0.017052578400506287, 'AGE'), (0.018941821085751577, 'RAD'), (0.023602561777571307, 'CHAS'), (0.025733049004581798, 'CRIM'), (0.031874162235100457, 'NOX'), (0.034405644939308928, 'INDUS'), (0.039713133345196064, 'DIS'), (0.046618521397262996, 'TAX'), (0.099511801492762245, 'PTRATIO'), (0.28421522796368465, 'LSTAT'), (0.35814513144036819, 'RM')]\n"
     ]
    }
   ],
   "source": [
    "print (sorted(zip(clf_et.feature_importances_, boston.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'LSTAT', 'RM' are by far the most influential features on our final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's evaluate the performance of our best method on the testing set\n",
    "(previously, we slightly modified our measure_performance function to show the\n",
    "coefficient of determination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True,\n",
    "    show_r2_score=False):\n",
    "    y_pred = clf.predict(X)\n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y, y_pred)),\"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y, y_pred),\"\\n\")\n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        print (\"Confusion matrix\")\n",
    "        print (metrics.confusion_matrix(y, y_pred),\"\\n\")\n",
    "\n",
    "    if show_r2_score:\n",
    "        print (\"Coefficient of determination:{0:.3f}\".format(metrics.r2_score(y, y_pred)),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination:0.802 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "measure_performance(X_test, y_test, clf_et, \n",
    "                    show_accuracy=False, \n",
    "                    show_classification_report=False, \n",
    "                    show_confusion_matrix=False, \n",
    "                    show_r2_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have selected our best method and used all the available data, we\n",
    "could train our best method on the whole training set, but we will have no way\n",
    "to measure its performance on future data, simply because we do not have any\n",
    "more data available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
