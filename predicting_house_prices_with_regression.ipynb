{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Python version: 3.5.2\n",
      "IPython version: 4.0.1\n",
      "numpy version: 1.13.1\n",
      "scikit-learn version: 0.18.2\n",
      "matplotlib version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import platform\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print ('Python version:', platform.python_version())\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare several regression methods by using the same\n",
    "dataset. We will try to predict the price of a house as a function of its attributes.\n",
    "As the dataset, we will use the Boston house-prices dataset, which includes 506\n",
    "instances, representing houses in the suburbs of Boston by 14 features, one of them\n",
    "(the median value of owner-occupied homes) being the target class (for a detailed\n",
    "reference, see http://archive.ics.uci.edu/ml/datasets/Housing). Each\n",
    "attribute in this dataset is real-valued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "50.0 5.0 22.5328063241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print (boston.data.shape)\n",
    "print (boston.feature_names)\n",
    "print (np.max(boston.target), np.min(boston.target), np.mean(boston.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start slicing our learning set into training and testing datasets, and\n",
    "normalizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.25, random_state=33)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "scalery = StandardScaler().fit(y_train)\n",
    "X_train = scalerX.transform(X_train)\n",
    "y_train = scalery.transform(y_train)\n",
    "X_test = scalerX.transform(X_test)\n",
    "y_test = scalery.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at our best classifier, let's define how we will compare our results.\n",
    "Since we want to preserve our testing set for evaluating the performance of the final\n",
    "classifier, we should find a way to select the best model while avoiding overfitting.\n",
    "We already know the answer: cross-validation. Regression poses an additional\n",
    "problem: how should we evaluate our results? Accuracy is not a good idea, since\n",
    "we are predicting real values, it is almost impossible for us to predict exactly the\n",
    "final value. There are several measures that can be used (you can look at the list of\n",
    "functions under sklearn.metrics module). The most common is the R2 score, or\n",
    "coefficient of determination that measures the proportion of the outcomes variation\n",
    "explained by the model, and is the default score function for regression methods in\n",
    "scikit-learn. This score reaches its maximum value of 1 when the model perfectly\n",
    "predicts all the test target values. Using this measure, we will build a function that\n",
    "trains a model and evaluates its performance using five-fold cross-validation and the\n",
    "coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import *\n",
    "\n",
    "def train_and_evaluate(clf, X_train, y_train):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print (\"Coefficient of determination on training set:\",clf.score(X_train, y_train))\n",
    "    # create a k-fold cross validation iterator of k=5 folds\n",
    "    cv = KFold(X_train.shape[0], 5, shuffle=True, random_state=33)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv)\n",
    "    print (\"Average coefficient of determination using 5-fold crossvalidation:\",np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try – a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question that linear models try to answer is which hyperplane in the\n",
    "14-dimensional space created by our learning features (including the target value)\n",
    "is located closer to them. After this hyperplane is found, prediction reduces to\n",
    "calculate the projection on the hyperplane of the new point, and returning the target\n",
    "value coordinate. Think of our first example in Chapter 1, Machine Learning – A Gentle\n",
    "Introduction, where we wanted to find a line separating our training instances.\n",
    "We could have used that line to predict the second learning attribute as a function\n",
    "of the first one, that is, linear regression.\n",
    "\n",
    "But, what do we mean by closer? The usual measure is least squares: calculate the\n",
    "distance of each instance to the hyperplane, square it (to avoid sign problems), and\n",
    "sum them. The hyperplane whose sum is smaller is the least squares estimator (the\n",
    "hyperplane in the case if two dimensions are just a line).\n",
    "\n",
    "Since we don't know how our data fits (it is difficult to print a 14-dimension\n",
    "scatter plot!), we will start with a linear model called SGDRegressor, which tries to\n",
    "minimize squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.743617732983\n",
      "Average coefficient of determination using 5-fold crossvalidation: 0.710809853468\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_sgd = linear_model.SGDRegressor(loss='squared_loss', penalty=None, random_state=42)\n",
    "train_and_evaluate(clf_sgd,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the hyperplane coefficients our method has calculated, which is\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08527595  0.06706144 -0.05032898  0.10874804 -0.07755151  0.38961893\n",
      " -0.02485839 -0.20990016  0.08491659 -0.05495108 -0.19854006  0.06126093\n",
      " -0.37817963]\n"
     ]
    }
   ],
   "source": [
    "print (clf_sgd.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
